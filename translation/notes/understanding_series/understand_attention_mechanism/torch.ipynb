{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce0ea79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0+cu113\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0ce1c0",
   "metadata": {},
   "source": [
    "The interactions between queries (volitional cues) and keys (nonvolitional cues) result in attention pooling. The attention pooling selectively aggregates values (sensory inputs) to produce the output. In this section, we will describe attention pooling in greater detail to give you a high-level view of how attention mechanisms work in practice. Specifically, the Nadaraya-Watson kernel regression model proposed in 1964 is a simple yet complete example for demonstrating machine learning with attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fd4ae4",
   "metadata": {},
   "source": [
    "# 1 intuition of attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4ae73c",
   "metadata": {},
   "source": [
    "## generate mock dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fa55ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train = 50 # train sample size\n",
    "x_train, _ = torch.sort(torch.rand(n_train) * 5)   # sorted train samples\n",
    "\n",
    "def f(x):\n",
    "    return 2 * torch.sin(x) + x**0.8\n",
    "\n",
    "y_train = f(x_train) + torch.normal(0.0, 0.5, (n_train,))  # ooutput of train samples\n",
    "x_test = torch.arange(0, 5, 0.5)  # test samples\n",
    "y_truth = f(x_test)  # true label of test samples\n",
    "n_test = len(x_test) \n",
    "n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ecfbf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1750, 0.3232, 0.3445, 0.6826, 0.6932, 0.7577, 0.7591, 0.8219, 0.8320,\n",
      "        0.9403, 0.9977, 1.0834, 1.0958, 1.1068, 1.3033, 1.6270, 1.8241, 1.8301,\n",
      "        1.9882, 2.0055, 2.0410, 2.3363, 2.4483, 2.6827, 2.6923, 2.7316, 2.8533,\n",
      "        2.8697, 2.8959, 3.2993, 3.3062, 3.3612, 3.3637, 3.5369, 3.6493, 3.7116,\n",
      "        3.7555, 3.9016, 3.9295, 3.9970, 4.1728, 4.2189, 4.2192, 4.2819, 4.3514,\n",
      "        4.5283, 4.6237, 4.6254, 4.7662, 4.9375])\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7f66370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 1.5332, 2.6829, 3.3782, 3.5597, 3.2783, 2.6905, 2.0227, 1.5178,\n",
      "        1.3759])\n"
     ]
    }
   ],
   "source": [
    "print(y_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df96c080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4839, 0.4465, 1.2327, 2.6117, 2.3442, 2.4821, 2.2916, 1.7639, 1.8538,\n",
      "        2.5228, 1.7965, 2.8513, 3.1080, 2.2103, 2.6587, 3.6102, 3.5720, 3.8102,\n",
      "        3.4114, 3.0347, 3.1894, 3.7735, 3.7699, 3.8172, 3.7964, 3.6540, 2.7537,\n",
      "        3.6843, 2.3706, 2.1517, 1.9694, 2.8241, 3.2272, 1.5330, 1.7377, 1.6393,\n",
      "        1.8550, 1.9915, 1.5142, 0.8626, 1.8244, 1.1411, 1.4603, 1.0822, 0.7710,\n",
      "        1.8000, 1.0719, 1.8532, 1.6951, 1.4659])\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2d2d1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.5000, 1.0000, 1.5000, 2.0000, 2.5000, 3.0000, 3.5000, 4.0000,\n",
      "        4.5000])\n"
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a7402",
   "metadata": {},
   "source": [
    "## 1.1 Average Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe7f60d",
   "metadata": {},
   "source": [
    "We begin with perhaps the world’s “dumbest” estimator for this regression problem: using average pooling to average over all the training outputs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e9753f",
   "metadata": {},
   "source": [
    "f(x) = \\frac{1}{n}\\sum_{i=1}^n y_i,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f3eca4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.2875, 2.2875, 2.2875, 2.2875, 2.2875, 2.2875, 2.2875, 2.2875, 2.2875,\n",
      "        2.2875])\n"
     ]
    }
   ],
   "source": [
    "y_hat = y_train.mean().repeat(n_test)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a030570",
   "metadata": {},
   "source": [
    "## 1.2 Nonparametric Attention Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3b5ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(queries, keys):\n",
    "    return queries.reshape((-1, 1)) - keys.reshape((1, -1)) #\n",
    "\n",
    "\n",
    "def score_function(queries, keys):\n",
    "    query_key_diffs = diff(queries, keys)\n",
    "    scores = - query_key_diffs**2 / 2\n",
    "    return scores\n",
    "\n",
    "def attention_pool(scores, values):\n",
    "    attention_weights = F.softmax(scores, dim=1)\n",
    "    return torch.matmul(attention_weights, values), attention_weights\n",
    "\n",
    "y_hat, attention_weights = attention_pool(score_function(x_test, x_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1500e0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1])\n",
      "torch.Size([1, 50])\n",
      "torch.Size([10, 50])\n"
     ]
    }
   ],
   "source": [
    "print(x_test.reshape((-1, 1)).shape)\n",
    "\n",
    "print(x_train.reshape((1, -1)).shape)\n",
    "\n",
    "a = x_test.reshape((-1, 1)) - x_train.reshape((1, -1))\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ab33ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0835, 2.2867, 2.5109, 2.7237, 2.8440, 2.7861, 2.5560, 2.2535, 1.9771,\n",
      "        1.7711])\n"
     ]
    }
   ],
   "source": [
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e355b9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.2032e-02, 7.9059e-02, 7.8498e-02, 6.5988e-02, 6.5506e-02, 6.2511e-02,\n",
      "         6.2446e-02, 5.9421e-02, 5.8928e-02, 5.3535e-02, 5.0640e-02, 4.6321e-02,\n",
      "         4.5696e-02, 4.5147e-02, 3.5626e-02, 2.2171e-02, 1.5780e-02, 1.5609e-02,\n",
      "         1.1541e-02, 1.1149e-02, 1.0377e-02, 5.4377e-03, 4.1590e-03, 2.2796e-03,\n",
      "         2.2217e-03, 1.9967e-03, 1.4217e-03, 1.3565e-03, 1.2578e-03, 3.6047e-04,\n",
      "         3.5234e-04, 2.9336e-04, 2.9085e-04, 1.6001e-04, 1.0684e-04, 8.4954e-05,\n",
      "         7.2121e-05, 4.1228e-05, 3.6959e-05, 2.8279e-05, 1.3793e-05, 1.1365e-05,\n",
      "         1.1353e-05, 8.6959e-06, 6.4432e-06, 2.9374e-06, 1.8981e-06, 1.8837e-06,\n",
      "         9.7218e-07, 4.2343e-07],\n",
      "        [5.5944e-02, 5.8064e-02, 5.8270e-02, 5.8004e-02, 5.7888e-02, 5.7052e-02,\n",
      "         5.7032e-02, 5.6001e-02, 5.5817e-02, 5.3531e-02, 5.2109e-02, 4.9751e-02,\n",
      "         4.9387e-02, 4.9062e-02, 4.2713e-02, 3.1252e-02, 2.4546e-02, 2.4353e-02,\n",
      "         1.9488e-02, 1.8990e-02, 1.7991e-02, 1.0927e-02, 8.8392e-03, 5.4472e-03,\n",
      "         5.3343e-03, 4.8895e-03, 3.6998e-03, 3.5590e-03, 3.3436e-03, 1.1724e-03,\n",
      "         1.1500e-03, 9.8412e-04, 9.7695e-04, 5.8609e-04, 4.1397e-04, 3.3957e-04,\n",
      "         2.9467e-04, 1.8121e-04, 1.6473e-04, 1.3037e-04, 6.9428e-05, 5.8542e-05,\n",
      "         5.8488e-05, 4.6227e-05, 3.5462e-05, 1.7662e-05, 1.1971e-05, 1.1890e-05,\n",
      "         6.5841e-06, 3.1241e-06],\n",
      "        [3.4626e-02, 3.8702e-02, 3.9257e-02, 4.6273e-02, 4.6427e-02, 4.7257e-02,\n",
      "         4.7272e-02, 4.7899e-02, 4.7982e-02, 4.8578e-02, 4.8664e-02, 4.8495e-02,\n",
      "         4.8441e-02, 4.8388e-02, 4.6476e-02, 3.9979e-02, 3.4652e-02, 3.4482e-02,\n",
      "         2.9864e-02, 2.9354e-02, 2.8307e-02, 1.9928e-02, 1.7050e-02, 1.1813e-02,\n",
      "         1.1624e-02, 1.0866e-02, 8.7379e-03, 8.4747e-03, 8.0667e-03, 3.4608e-03,\n",
      "         3.4062e-03, 2.9962e-03, 2.9782e-03, 1.9483e-03, 1.4557e-03, 1.2318e-03,\n",
      "         1.0926e-03, 7.2284e-04, 6.6635e-04, 5.4548e-04, 3.1717e-04, 2.7368e-04,\n",
      "         2.7346e-04, 2.2302e-04, 1.7713e-04, 9.6383e-05, 6.8518e-05, 6.8108e-05,\n",
      "         4.0469e-05, 2.0919e-05],\n",
      "        [1.8567e-02, 2.2349e-02, 2.2912e-02, 3.1980e-02, 3.2258e-02, 3.3911e-02,\n",
      "         3.3945e-02, 3.5492e-02, 3.5734e-02, 3.8190e-02, 3.9372e-02, 4.0953e-02,\n",
      "         4.1163e-02, 4.1343e-02, 4.3811e-02, 4.4307e-02, 4.2381e-02, 4.2298e-02,\n",
      "         3.9648e-02, 3.9309e-02, 3.8586e-02, 3.1486e-02, 2.8490e-02, 2.2194e-02,\n",
      "         2.1943e-02, 2.0921e-02, 1.7878e-02, 1.7483e-02, 1.6860e-02, 8.8502e-03,\n",
      "         8.7407e-03, 7.9028e-03, 7.8652e-03, 5.6108e-03, 4.4345e-03, 3.8713e-03,\n",
      "         3.5100e-03, 2.4980e-03, 2.3351e-03, 1.9772e-03, 1.2553e-03, 1.1084e-03,\n",
      "         1.1077e-03, 9.3215e-04, 7.6652e-04, 4.5566e-04, 3.3975e-04, 3.3800e-04,\n",
      "         2.1549e-04, 1.2135e-04],\n",
      "        [8.1465e-03, 1.0560e-02, 1.0942e-02, 1.8086e-02, 1.8340e-02, 1.9912e-02,\n",
      "         1.9946e-02, 2.1521e-02, 2.1776e-02, 2.4568e-02, 2.6066e-02, 2.8299e-02,\n",
      "         2.8622e-02, 2.8906e-02, 3.3794e-02, 4.0181e-02, 4.2414e-02, 4.2458e-02,\n",
      "         4.3072e-02, 4.3075e-02, 4.3039e-02, 4.0708e-02, 3.8957e-02, 3.4121e-02,\n",
      "         3.3897e-02, 3.2960e-02, 2.9932e-02, 2.9511e-02, 2.8837e-02, 1.8520e-02,\n",
      "         1.8354e-02, 1.7057e-02, 1.6997e-02, 1.3222e-02, 1.1054e-02, 9.9556e-03,\n",
      "         9.2266e-03, 7.0639e-03, 6.6962e-03, 5.8646e-03, 4.0652e-03, 3.6735e-03,\n",
      "         3.6715e-03, 3.1881e-03, 2.7143e-03, 1.7627e-03, 1.3786e-03, 1.3726e-03,\n",
      "         9.3894e-04, 5.7605e-04],\n",
      "        [2.7987e-03, 3.9069e-03, 4.0918e-03, 8.0082e-03, 8.1643e-03, 9.1547e-03,\n",
      "         9.1764e-03, 1.0217e-02, 1.0390e-02, 1.2375e-02, 1.3511e-02, 1.5311e-02,\n",
      "         1.5583e-02, 1.5824e-02, 2.0410e-02, 2.8531e-02, 3.3235e-02, 3.3369e-02,\n",
      "         3.6637e-02, 3.6957e-02, 3.7587e-02, 4.1207e-02, 4.1708e-02, 4.1072e-02,\n",
      "         4.0999e-02, 4.0658e-02, 3.9237e-02, 3.9005e-02, 3.8616e-02, 3.0343e-02,\n",
      "         3.0175e-02, 2.8824e-02, 2.8760e-02, 2.4396e-02, 2.1575e-02, 2.0046e-02,\n",
      "         1.8990e-02, 1.5640e-02, 1.5034e-02, 1.3620e-02, 1.0308e-02, 9.5322e-03,\n",
      "         9.5281e-03, 8.5373e-03, 7.5252e-03, 5.3392e-03, 4.3797e-03, 4.3643e-03,\n",
      "         3.2032e-03, 2.1410e-03],\n",
      "        [7.4864e-04, 1.1255e-03, 1.1914e-03, 2.7611e-03, 2.8299e-03, 3.2773e-03,\n",
      "         3.2873e-03, 3.7768e-03, 3.8603e-03, 4.8535e-03, 5.4533e-03, 6.4503e-03,\n",
      "         6.6058e-03, 6.7448e-03, 9.5982e-03, 1.5774e-02, 2.0278e-02, 2.0420e-02,\n",
      "         2.4265e-02, 2.4690e-02, 2.5560e-02, 3.2480e-02, 3.4769e-02, 3.8496e-02,\n",
      "         3.8612e-02, 3.9052e-02, 4.0050e-02, 4.0141e-02, 4.0265e-02, 3.8710e-02,\n",
      "         3.8629e-02, 3.7927e-02, 3.7892e-02, 3.5049e-02, 3.2789e-02, 3.1428e-02,\n",
      "         3.0433e-02, 2.6964e-02, 2.6284e-02, 2.4628e-02, 2.0352e-02, 1.9260e-02,\n",
      "         1.9254e-02, 1.7801e-02, 1.6246e-02, 1.2592e-02, 1.0834e-02, 1.0805e-02,\n",
      "         8.5092e-03, 6.1960e-03],\n",
      "        [1.6084e-04, 2.6040e-04, 2.7861e-04, 7.6458e-04, 7.8782e-04, 9.4228e-04,\n",
      "         9.4579e-04, 1.1213e-03, 1.1519e-03, 1.5289e-03, 1.7678e-03, 2.1825e-03,\n",
      "         2.2491e-03, 2.3090e-03, 3.6252e-03, 7.0046e-03, 9.9370e-03, 1.0036e-02,\n",
      "         1.2908e-02, 1.3248e-02, 1.3960e-02, 2.0562e-02, 2.3279e-02, 2.8979e-02,\n",
      "         2.9205e-02, 3.0125e-02, 3.2833e-02, 3.3179e-02, 3.3720e-02, 3.9663e-02,\n",
      "         3.9717e-02, 4.0082e-02, 4.0096e-02, 4.0443e-02, 4.0021e-02, 3.9574e-02,\n",
      "         3.9171e-02, 3.7335e-02, 3.6905e-02, 3.5768e-02, 3.2274e-02, 3.1254e-02,\n",
      "         3.1248e-02, 2.9811e-02, 2.8167e-02, 2.3852e-02, 2.1525e-02, 2.1485e-02,\n",
      "         1.8155e-02, 1.4402e-02],\n",
      "        [2.9011e-05, 5.0582e-05, 5.4699e-05, 1.7775e-04, 1.8413e-04, 2.2745e-04,\n",
      "         2.2846e-04, 2.7950e-04, 2.8857e-04, 4.0432e-04, 4.8111e-04, 6.1998e-04,\n",
      "         6.4289e-04, 6.6365e-04, 1.1495e-03, 2.6114e-03, 4.0882e-03, 4.1414e-03,\n",
      "         5.7644e-03, 5.9676e-03, 6.4011e-03, 1.0928e-02, 1.3085e-02, 1.8315e-02,\n",
      "         1.8546e-02, 1.9511e-02, 2.2597e-02, 2.3024e-02, 2.3708e-02, 3.4120e-02,\n",
      "         3.4284e-02, 3.5563e-02, 3.5621e-02, 3.9179e-02, 4.1012e-02, 4.1837e-02,\n",
      "         4.2328e-02, 4.3402e-02, 4.3504e-02, 4.3613e-02, 4.2967e-02, 4.2580e-02,\n",
      "         4.2578e-02, 4.1914e-02, 4.1002e-02, 3.7933e-02, 3.5904e-02, 3.5867e-02,\n",
      "         3.2519e-02, 2.8103e-02],\n",
      "        [4.5763e-06, 8.5928e-06, 9.3917e-06, 3.6140e-05, 3.7637e-05, 4.8016e-05,\n",
      "         4.8261e-05, 6.0927e-05, 6.3221e-05, 9.3511e-05, 1.1451e-04, 1.5402e-04,\n",
      "         1.6071e-04, 1.6681e-04, 3.1879e-04, 8.5139e-04, 1.4709e-03, 1.4945e-03,\n",
      "         2.2514e-03, 2.3510e-03, 2.5669e-03, 5.0794e-03, 6.4327e-03, 1.0123e-02,\n",
      "         1.0300e-02, 1.1051e-02, 1.3602e-02, 1.3973e-02, 1.4578e-02, 2.5668e-02,\n",
      "         2.5882e-02, 2.7595e-02, 2.7675e-02, 3.3192e-02, 3.6754e-02, 3.8679e-02,\n",
      "         4.0002e-02, 4.4124e-02, 4.4850e-02, 4.6506e-02, 5.0026e-02, 5.0733e-02,\n",
      "         5.0736e-02, 5.1537e-02, 5.2197e-02, 5.2756e-02, 5.2375e-02, 5.2364e-02,\n",
      "         5.0940e-02, 4.7960e-02]])\n",
      "torch.Size([10, 50])\n"
     ]
    }
   ],
   "source": [
    "print(attention_weights)\n",
    "print(attention_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15bf177",
   "metadata": {},
   "source": [
    "## 1.3 Parametric Attention Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43b412d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1750, 0.3232, 0.3445,  ..., 4.6254, 4.7662, 4.9375],\n",
      "        [0.1750, 0.3232, 0.3445,  ..., 4.6254, 4.7662, 4.9375],\n",
      "        [0.1750, 0.3232, 0.3445,  ..., 4.6254, 4.7662, 4.9375],\n",
      "        ...,\n",
      "        [0.1750, 0.3232, 0.3445,  ..., 4.6254, 4.7662, 4.9375],\n",
      "        [0.1750, 0.3232, 0.3445,  ..., 4.6254, 4.7662, 4.9375],\n",
      "        [0.1750, 0.3232, 0.3445,  ..., 4.6254, 4.7662, 4.9375]])\n",
      "torch.Size([50, 50])\n"
     ]
    }
   ],
   "source": [
    "#(n_train，n_train)\n",
    "X_tile = x_train.repeat((n_train, 1))\n",
    "print(X_tile)\n",
    "print(X_tile.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3c57b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4839, 0.4465, 1.2327,  ..., 1.8532, 1.6951, 1.4659],\n",
      "        [0.4839, 0.4465, 1.2327,  ..., 1.8532, 1.6951, 1.4659],\n",
      "        [0.4839, 0.4465, 1.2327,  ..., 1.8532, 1.6951, 1.4659],\n",
      "        ...,\n",
      "        [0.4839, 0.4465, 1.2327,  ..., 1.8532, 1.6951, 1.4659],\n",
      "        [0.4839, 0.4465, 1.2327,  ..., 1.8532, 1.6951, 1.4659],\n",
      "        [0.4839, 0.4465, 1.2327,  ..., 1.8532, 1.6951, 1.4659]])\n",
      "torch.Size([50, 50])\n"
     ]
    }
   ],
   "source": [
    "#(n_train，n_train)\n",
    "Y_tile = y_train.repeat((n_train, 1))\n",
    "print(Y_tile)\n",
    "print(Y_tile.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "285f0694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3232, 0.3445, 0.6826,  ..., 4.6254, 4.7662, 4.9375],\n",
      "        [0.1750, 0.3445, 0.6826,  ..., 4.6254, 4.7662, 4.9375],\n",
      "        [0.1750, 0.3232, 0.6826,  ..., 4.6254, 4.7662, 4.9375],\n",
      "        ...,\n",
      "        [0.1750, 0.3232, 0.3445,  ..., 4.6237, 4.7662, 4.9375],\n",
      "        [0.1750, 0.3232, 0.3445,  ..., 4.6237, 4.6254, 4.9375],\n",
      "        [0.1750, 0.3232, 0.3445,  ..., 4.6237, 4.6254, 4.7662]])\n",
      "torch.Size([50, 49])\n"
     ]
    }
   ],
   "source": [
    "# keys的形状:('n_train'，'n_train'-1)\n",
    "keys = X_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape((n_train, -1))\n",
    "print(keys)\n",
    "print(keys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b27f92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4465, 1.2327, 2.6117,  ..., 1.8532, 1.6951, 1.4659],\n",
      "        [0.4839, 1.2327, 2.6117,  ..., 1.8532, 1.6951, 1.4659],\n",
      "        [0.4839, 0.4465, 2.6117,  ..., 1.8532, 1.6951, 1.4659],\n",
      "        ...,\n",
      "        [0.4839, 0.4465, 1.2327,  ..., 1.0719, 1.6951, 1.4659],\n",
      "        [0.4839, 0.4465, 1.2327,  ..., 1.0719, 1.8532, 1.4659],\n",
      "        [0.4839, 0.4465, 1.2327,  ..., 1.0719, 1.8532, 1.6951]])\n",
      "torch.Size([50, 49])\n"
     ]
    }
   ],
   "source": [
    "# values的形状:('n_train'，'n_train'-1)\n",
    "values = Y_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape((n_train, -1))\n",
    "print(values)\n",
    "print(values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1c12da",
   "metadata": {},
   "source": [
    "### train process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e07846f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_function_with_parameter(queries, keys, w):\n",
    "    return -((queries - keys) * w)**2 / 2\n",
    "    \n",
    "\n",
    "class NWKernelRegression(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.w = nn.Parameter(torch.rand((1,), requires_grad=True))\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # queries和attention_weights的形状为(查询个数，“键－值”对个数)\n",
    "        queries = queries.repeat_interleave(keys.shape[1]).reshape((-1, keys.shape[1]))\n",
    "        scores = score_function_with_parameter(queries,keys, self.w)\n",
    "        self.attention_weights = nn.functional.softmax(scores, dim=1)\n",
    "        # values的形状为(查询个数，“键－值”对个数)\n",
    "        return torch.bmm(self.attention_weights.unsqueeze(1),values.unsqueeze(-1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7773310f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 31.119806\n",
      "epoch 2, loss 10.461807\n",
      "epoch 3, loss 10.461460\n",
      "epoch 4, loss 10.461108\n",
      "epoch 5, loss 10.460758\n"
     ]
    }
   ],
   "source": [
    "net = NWKernelRegression()\n",
    "loss = nn.MSELoss(reduction='none')\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.5)\n",
    "\n",
    "for epoch in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    l = loss(net(x_train, keys, values), y_train)\n",
    "    l.sum().backward()\n",
    "    optimizer.step()\n",
    "    print(f'epoch {epoch + 1}, loss {float(l.sum()):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb665bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([17.1402], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(net.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e33fd249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.2999e-01, 2.7001e-01, 7.1064e-16,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [4.0903e-02, 9.5910e-01, 6.0871e-09,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [1.5529e-02, 9.8447e-01, 5.5434e-08,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.6590e-01, 4.1745e-02,\n",
      "         4.7585e-07],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.2702e-01, 4.5744e-01,\n",
      "         1.1346e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.9473e-05, 4.5953e-05,\n",
      "         9.9991e-01]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([50, 49])\n"
     ]
    }
   ],
   "source": [
    "print(net.attention_weights)\n",
    "print(net.attention_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8539b26",
   "metadata": {},
   "source": [
    "### evaluate process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b7bbe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys的形状:(n_test，n_train)，每一行包含着相同的训练输入（例如，相同的键）\n",
    "keys = x_train.repeat((n_test, 1))\n",
    "# value的形状:(n_test，n_train)\n",
    "values = y_train.repeat((n_test, 1))\n",
    "y_hat = net(x_test, keys, values).unsqueeze(1).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3822dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([17.1402], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(net.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "240cfdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9998e-01, 1.9499e-05, 2.4042e-06, 1.7062e-28, 1.9839e-29, 2.1132e-35,\n",
      "         1.5621e-35, 7.1901e-42, 6.2358e-43, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [3.6027e-06, 2.0024e-01, 5.6749e-01, 1.4791e-01, 8.2179e-02, 1.1443e-03,\n",
      "         1.0324e-03, 4.8421e-06, 1.8423e-06, 8.4895e-12, 3.1332e-15, 3.8617e-21,\n",
      "         4.4563e-22, 6.4202e-23, 1.3355e-40, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.5414e-44, 2.4654e-30, 1.6060e-28, 1.5374e-07, 4.0814e-07, 7.4295e-05,\n",
      "         8.1801e-05, 3.9102e-03, 6.5212e-03, 2.4432e-01, 4.1212e-01, 1.4865e-01,\n",
      "         1.0707e-01, 7.7252e-02, 5.5621e-07, 3.4109e-26, 1.9618e-44, 4.2039e-45,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4299e-42, 3.0815e-41, 7.3328e-35,\n",
      "         9.8532e-35, 4.8002e-29, 3.5090e-28, 1.0689e-19, 8.2405e-16, 8.6984e-11,\n",
      "         3.9107e-10, 1.4131e-09, 3.5214e-02, 9.6478e-01, 2.0536e-06, 1.1590e-06,\n",
      "         6.4317e-15, 5.1531e-16, 2.2079e-18, 2.5223e-44, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 3.9255e-32, 4.8049e-10, 3.8196e-03, 5.1684e-03,\n",
      "         3.5222e-01, 3.5789e-01, 2.8090e-01, 2.1990e-08, 5.4114e-14, 6.6513e-31,\n",
      "         9.6153e-32, 2.5539e-35, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0166e-29, 3.2982e-29,\n",
      "         2.7602e-17, 3.5568e-16, 5.1143e-14, 2.7551e-02, 9.5522e-01, 1.0501e-02,\n",
      "         6.1968e-03, 5.3413e-04, 1.5474e-08, 2.7035e-09, 1.4193e-10, 2.4604e-41,\n",
      "         4.8275e-42, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3936e-28, 1.1693e-19, 1.1496e-06,\n",
      "         2.7694e-06, 7.7467e-05, 1.2880e-01, 2.5136e-01, 6.1974e-01, 5.8630e-06,\n",
      "         3.1721e-06, 1.4503e-08, 1.1046e-08, 1.2369e-18, 3.8541e-27, 1.5062e-32,\n",
      "         1.1821e-36, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4523e-43,\n",
      "         2.4158e-42, 2.1933e-38, 2.0930e-27, 4.5624e-26, 5.2826e-24, 2.7273e-03,\n",
      "         4.0692e-03, 5.9641e-02, 6.6133e-02, 8.2773e-01, 3.8220e-02, 1.4060e-03,\n",
      "         6.9333e-05, 5.2270e-11, 1.7318e-12, 1.7637e-16, 1.3471e-29, 1.0775e-33,\n",
      "         1.0226e-33, 1.0070e-39, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7572e-32,\n",
      "         1.1345e-31, 5.3305e-27, 8.6053e-27, 1.2039e-14, 8.2371e-09, 2.8522e-06,\n",
      "         8.8377e-05, 1.3878e-01, 2.7747e-01, 5.7548e-01, 7.1808e-03, 5.0474e-04,\n",
      "         4.9675e-04, 4.9098e-06, 7.6750e-09, 9.0564e-19, 8.7966e-26, 6.4922e-26,\n",
      "         2.0324e-38, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9677e-40,\n",
      "         3.8310e-36, 1.2530e-23, 1.5118e-21, 6.3858e-17, 1.3018e-07, 8.0403e-06,\n",
      "         8.2066e-06, 8.1411e-04, 3.4337e-02, 7.8399e-01, 9.3144e-02, 8.7670e-02,\n",
      "         2.6565e-05, 5.4147e-13]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([10, 50])\n"
     ]
    }
   ],
   "source": [
    "print(net.attention_weights)\n",
    "print(net.attention_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a0f558",
   "metadata": {},
   "source": [
    "# 2 general framework of attention mechanisms - Attention Scoring Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92833a6",
   "metadata": {},
   "source": [
    "In Section 1, we used a Gaussian kernel to model interactions between queries and keys. Treating the exponent of the Gaussian kernel in (11.2.6) as an attention scoring function (or scoring function for short), the results of this function were essentially fed into a softmax operation. As a result, we obtained a probability distribution (attention weights) over values that are paired with keys. In the end, the output of the attention pooling is simply a weighted sum of the values based on these attention weights.\n",
    "\n",
    "At a high level, we can use the above algorithm to instantiate the framework of attention mechanisms in Fig. 11.1.3. Denoting an attention scoring function by , Fig. 11.3.1 illustrates how the output of attention pooling can be computed as a weighted sum of values. Since attention weights are a probability distribution, the weighted sum is essentially a weighted average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c668c04e",
   "metadata": {},
   "source": [
    "![](https://d2l.ai/_images/attention-output.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6a5e08",
   "metadata": {},
   "source": [
    "Mathematically, suppose that we have a query \\mathbf{q} \\in \\mathbb{R}^q and  key-value pairs , where any  and any . The attention pooling  is instantiated as a weighted sum of the values:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37180d7",
   "metadata": {},
   "source": [
    "## elementary knowledge - Masked Softmax Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b057e240",
   "metadata": {},
   "source": [
    "Ａs we just mentioned, a softmax operation is used to output a probability distribution as attention weights. In some cases, not all the values should be fed into attention pooling. For instance, for efficient minibatch processing in Section 10.5, some text sequences are padded with special tokens that do not carry meaning. To get an attention pooling over only meaningful tokens as values, we can specify a valid sequence length (in number of tokens) to filter out those beyond this specified range when computing softmax. In this way, we can implement such a masked softmax operation in the following masked_softmax function, where any value beyond the valid length is masked as zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09164f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: 3D tensor, valid_lens: 1D or 2D tensor\n",
    "def sequence_mask(attention_scores, valid_len, value=0):\n",
    "    maxlen = attention_scores.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32, device=attention_scores.device)[None, :] < valid_len[:, None]\n",
    "    attention_scores[~mask] = value\n",
    "    return attention_scores\n",
    "\n",
    "    \n",
    "def masked_softmax(attention_scores, valid_lens):\n",
    "    \"\"\"Perform softmax operation by masking elements on the last axis.\n",
    "    Defined in :numref:`sec_attention-scoring-functions`\"\"\"\n",
    "    \n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(attention_scores, dim=-1)\n",
    "    else:\n",
    "        shape = attention_scores.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # On the last axis, replace masked elements with a very large negative\n",
    "        # value, whose exponentiation outputs 0\n",
    "        attention_scores = sequence_mask(attention_scores.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
    "        return nn.functional.softmax(attention_scores.reshape(shape), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e256c98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7113, 0.2887, 0.0000, 0.0000],\n",
      "         [0.4018, 0.5982, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.4104, 0.2470, 0.3426, 0.0000],\n",
      "         [0.2497, 0.4913, 0.2590, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "masked_attention_weights = masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))\n",
    "print(masked_attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15c460ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4249, 0.1644, 0.4107, 0.0000]],\n",
      "\n",
      "        [[0.6734, 0.3266, 0.0000, 0.0000],\n",
      "         [0.2495, 0.1832, 0.3178, 0.2495]]])\n"
     ]
    }
   ],
   "source": [
    "masked_attention_weights = masked_softmax(torch.rand(2, 2, 4), torch.tensor([[1, 3], [2, 4]]))\n",
    "print(masked_attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80859c1",
   "metadata": {},
   "source": [
    "## 2.1 Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71137c7a",
   "metadata": {},
   "source": [
    "A more computationally efficient design for the scoring function can be simply dot product. However, the dot product operation requires that both the query and the key have the same vector length, say d. Assume that all the elements of the query and the key are independent random variables with zero mean and unit variance. The dot product of both vectors has zero mean and a variance of . To ensure that the variance of the dot product still remains one regardless of vector length, the scaled dot-product attention scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "184aee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"Scaled dot product attention.\"\"\"\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    # Shape of queries: (batch_size, no. of queries, d)\n",
    "    # Shape of keys: (batch_size, no. of key-value pairs, d)\n",
    "    # Shape of values: (batch_size, no. of key-value pairs, value dimension)\n",
    "    # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        d = queries.shape[-1]\n",
    "        # Swap the last two dimensions of keys with keys.transpose(1, 2)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cc58bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2017, -0.5536]],\n",
      "\n",
      "        [[ 1.9334,  1.4100]]])\n",
      "tensor([[[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]]])\n",
      "tensor([[[ 0.,  1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.,  7.],\n",
      "         [ 8.,  9., 10., 11.],\n",
      "         [12., 13., 14., 15.],\n",
      "         [16., 17., 18., 19.],\n",
      "         [20., 21., 22., 23.],\n",
      "         [24., 25., 26., 27.],\n",
      "         [28., 29., 30., 31.],\n",
      "         [32., 33., 34., 35.],\n",
      "         [36., 37., 38., 39.]],\n",
      "\n",
      "        [[ 0.,  1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.,  7.],\n",
      "         [ 8.,  9., 10., 11.],\n",
      "         [12., 13., 14., 15.],\n",
      "         [16., 17., 18., 19.],\n",
      "         [20., 21., 22., 23.],\n",
      "         [24., 25., 26., 27.],\n",
      "         [28., 29., 30., 31.],\n",
      "         [32., 33., 34., 35.],\n",
      "         [36., 37., 38., 39.]]])\n"
     ]
    }
   ],
   "source": [
    "queries, keys = torch.normal(0, 1, (2, 1, 2)), torch.ones((2, 10, 2))\n",
    "\n",
    "print(queries)\n",
    "\n",
    "print(keys)\n",
    "\n",
    "# The two value matrices in the values minibatch are identical\n",
    "values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(2, 1, 1)\n",
    "print(values)\n",
    "\n",
    "valid_lens = torch.tensor([2, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c56c0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
      "\n",
      "        [[10.0000, 11.0000, 12.0000, 13.0000]]])\n",
      "torch.Size([2, 1, 4])\n",
      "tensor([[[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000]]])\n",
      "torch.Size([2, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "attention = DotProductAttention(dropout=0.5)\n",
    "attention.eval()\n",
    "output = attention(queries, keys, values, valid_lens)\n",
    "print(output)\n",
    "print(output.shape)\n",
    "\n",
    "print(attention.attention_weights)\n",
    "print(attention.attention_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521e41e4",
   "metadata": {},
   "source": [
    "## 2.2 Additive Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b35c58",
   "metadata": {},
   "source": [
    "In general, when queries and keys are vectors of different lengths, we can use additive attention as the scoring function. Given a query  and a key , the additive attention scoring function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ad5385",
   "metadata": {},
   "source": [
    "a(\\mathbf q, \\mathbf k) = \\mathbf w_v^\\top \\text{tanh}(\\mathbf W_q\\mathbf q + \\mathbf W_k \\mathbf k) \\in \\mathbb{R},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "660f5dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"Additive attention.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, **kwargs):\n",
    "        super(AdditiveAttention, self).__init__(**kwargs)\n",
    "        self.W_k = nn.LazyLinear(num_hiddens, bias=False)\n",
    "        self.W_q = nn.LazyLinear(num_hiddens, bias=False)\n",
    "        self.w_v = nn.LazyLinear(1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
    "        # After dimension expansion, shape of queries: (batch_size, no. of\n",
    "        # queries, 1, num_hiddens) and shape of keys: (batch_size, 1, no. of\n",
    "        # key-value pairs, num_hiddens). Sum them up with broadcasting\n",
    "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
    "        features = torch.tanh(features)\n",
    "        # There is only one output of self.w_v, so we remove the last\n",
    "        # one-dimensional entry from the shape. Shape of scores: (batch_size,\n",
    "        # no. of queries, no. of key-value pairs)\n",
    "        scores = self.w_v(features).squeeze(-1)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        # Shape of values: (batch_size, no. of key-value pairs, value\n",
    "        # dimension)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6d5e441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
      "\n",
      "        [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=<BmmBackward0>)\n",
      "torch.Size([2, 1, 4])\n",
      "tensor([[[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([2, 1, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sz/anaconda3/envs/cling/lib/python3.10/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "queries, keys = torch.normal(0, 1, (2, 1, 20)), torch.ones((2, 10, 2))\n",
    "# The two value matrices in the values minibatch are identical\n",
    "values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(\n",
    "    2, 1, 1)\n",
    "valid_lens = torch.tensor([2, 6])\n",
    "\n",
    "attention = AdditiveAttention(num_hiddens=8, dropout=0.1)\n",
    "attention.eval()\n",
    "output = attention(queries, keys, values, valid_lens)\n",
    "print(output)\n",
    "print(output.shape)\n",
    "\n",
    "print(attention.attention_weights)\n",
    "print(attention.attention_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ddb9c3",
   "metadata": {},
   "source": [
    "# further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6879299c",
   "metadata": {},
   "source": [
    "## multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7f41c4",
   "metadata": {},
   "source": [
    "In practice, given the same set of queries, keys, and values we may want our model to combine knowledge from different behaviors of the same attention mechanism, such as capturing dependencies of various ranges (e.g., shorter-range vs. longer-range) within a sequence. Thus, it may be beneficial to allow our attention mechanism to jointly use different representation subspaces of queries, keys, and values.\n",
    "\n",
    "To this end, instead of performing a single attention pooling, queries, keys, and values can be transformed with  independently learned linear projections. Then these  projected queries, keys, and values are fed into attention pooling in parallel. In the end,  attention pooling outputs are concatenated and transformed with another learned linear projection to produce the final output. This design is called multi-head attention, where each of the  attention pooling outputs is a head [Vaswani et al., 2017]. Using fully connected layers to perform learnable linear transformations, Fig. 11.5.1 describes multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49435aa",
   "metadata": {},
   "source": [
    "![](https://d2l.ai/_images/multi-head-attention.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84ce0e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_qkv(X, num_heads):\n",
    "    \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n",
    "    # Shape of input X: (batch_size, no. of queries or key-value pairs,\n",
    "    # num_hiddens). Shape of output X: (batch_size, no. of queries or\n",
    "    # key-value pairs, num_heads, num_hiddens / num_heads)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "    # Shape of output X: (batch_size, num_heads, no. of queries or key-value\n",
    "    # pairs, num_hiddens / num_heads)\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    # Shape of output: (batch_size * num_heads, no. of queries or key-value\n",
    "    # pairs, num_hiddens / num_heads)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "def transpose_output(X, num_heads):\n",
    "    \"\"\"Reverse the operation of transpose_qkv.\"\"\"\n",
    "    X = X.reshape(-1,num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "299c358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 num_heads, dropout, bias=False, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)\n",
    "        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)\n",
    "        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        # queries，keys，values:\n",
    "        # (batch_size，查询或者“键－值”对的个数，num_hiddens)\n",
    "        # valid_lens　的形状:\n",
    "        # (batch_size，)或(batch_size，查询的个数)\n",
    "        # 经过变换后，输出的queries，keys，values　的形状:\n",
    "        # (batch_size*num_heads，查询或者“键－值”对的个数，\n",
    "        # num_hiddens/num_heads)\n",
    "        queries = transpose_qkv(self.W_q(queries), self.num_heads)\n",
    "        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n",
    "        values = transpose_qkv(self.W_v(values), self.num_heads)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            # 在轴0，将第一项（标量或者矢量）复制num_heads次，\n",
    "            # 然后如此复制第二项，然后诸如此类。\n",
    "            valid_lens = torch.repeat_interleave(\n",
    "                valid_lens, repeats=self.num_heads, dim=0)\n",
    "\n",
    "        # output的形状:(batch_size*num_heads，查询的个数，\n",
    "        # num_hiddens/num_heads)\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "\n",
    "        # output_concat的形状:(batch_size，查询的个数，num_hiddens)\n",
    "        output_concat = transpose_output(output, self.num_heads)\n",
    "        return self.W_o(output_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "778773e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (attention): DotProductAttention(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (W_q): Linear(in_features=10, out_features=10, bias=False)\n",
       "  (W_k): Linear(in_features=10, out_features=10, bias=False)\n",
       "  (W_v): Linear(in_features=10, out_features=10, bias=False)\n",
       "  (W_o): Linear(in_features=10, out_features=10, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens, num_heads = 10, 5\n",
    "multi_head_attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens, num_hiddens, num_heads, 0.5)\n",
    "multi_head_attention.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2fb2ddfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5506, -0.1278, -0.5473,  0.1268,  0.0913,  0.0385, -0.0627,\n",
      "           0.3436,  0.0286,  0.2243],\n",
      "         [ 0.5506, -0.1278, -0.5473,  0.1268,  0.0913,  0.0385, -0.0627,\n",
      "           0.3436,  0.0286,  0.2243],\n",
      "         [ 0.5506, -0.1278, -0.5473,  0.1268,  0.0913,  0.0385, -0.0627,\n",
      "           0.3436,  0.0286,  0.2243],\n",
      "         [ 0.5506, -0.1278, -0.5473,  0.1268,  0.0913,  0.0385, -0.0627,\n",
      "           0.3436,  0.0286,  0.2243]],\n",
      "\n",
      "        [[ 0.5506, -0.1278, -0.5473,  0.1268,  0.0913,  0.0385, -0.0627,\n",
      "           0.3436,  0.0286,  0.2243],\n",
      "         [ 0.5506, -0.1278, -0.5473,  0.1268,  0.0913,  0.0385, -0.0627,\n",
      "           0.3436,  0.0286,  0.2243],\n",
      "         [ 0.5506, -0.1278, -0.5473,  0.1268,  0.0913,  0.0385, -0.0627,\n",
      "           0.3436,  0.0286,  0.2243],\n",
      "         [ 0.5506, -0.1278, -0.5473,  0.1268,  0.0913,  0.0385, -0.0627,\n",
      "           0.3436,  0.0286,  0.2243]]], grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([2, 4, 10])\n"
     ]
    }
   ],
   "source": [
    "batch_size, num_queries = 2, 4\n",
    "num_kvpairs, valid_lens =  6, torch.tensor([3, 2])\n",
    "\n",
    "X = torch.ones((batch_size, num_queries, num_hiddens))\n",
    "Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n",
    "output = multi_head_attention(X, Y, Y, valid_lens)\n",
    "\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52ba70e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([10, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "print(multi_head_attention.attention.attention_weights)\n",
    "print(multi_head_attention.attention.attention_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d082934",
   "metadata": {},
   "source": [
    "## self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b8db45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (attention): DotProductAttention(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (W_q): Linear(in_features=10, out_features=10, bias=False)\n",
       "  (W_k): Linear(in_features=10, out_features=10, bias=False)\n",
       "  (W_v): Linear(in_features=10, out_features=10, bias=False)\n",
       "  (W_o): Linear(in_features=10, out_features=10, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens, num_heads = 10, 5\n",
    "multi_head_attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,\n",
    "                                   num_hiddens, num_heads, 0.5)\n",
    "multi_head_attention.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef22b453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4852, -0.4486,  0.0234, -0.2439, -0.1019, -0.6457, -0.4172,\n",
      "           0.2136, -0.6824,  0.4824],\n",
      "         [ 0.4852, -0.4486,  0.0234, -0.2439, -0.1019, -0.6457, -0.4172,\n",
      "           0.2136, -0.6824,  0.4824],\n",
      "         [ 0.4852, -0.4486,  0.0234, -0.2439, -0.1019, -0.6457, -0.4172,\n",
      "           0.2136, -0.6824,  0.4824],\n",
      "         [ 0.4852, -0.4486,  0.0234, -0.2439, -0.1019, -0.6457, -0.4172,\n",
      "           0.2136, -0.6824,  0.4824]],\n",
      "\n",
      "        [[ 0.4852, -0.4486,  0.0234, -0.2439, -0.1019, -0.6457, -0.4172,\n",
      "           0.2136, -0.6824,  0.4824],\n",
      "         [ 0.4852, -0.4486,  0.0234, -0.2439, -0.1019, -0.6457, -0.4172,\n",
      "           0.2136, -0.6824,  0.4824],\n",
      "         [ 0.4852, -0.4486,  0.0234, -0.2439, -0.1019, -0.6457, -0.4172,\n",
      "           0.2136, -0.6824,  0.4824],\n",
      "         [ 0.4852, -0.4486,  0.0234, -0.2439, -0.1019, -0.6457, -0.4172,\n",
      "           0.2136, -0.6824,  0.4824]]], grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([2, 4, 10])\n"
     ]
    }
   ],
   "source": [
    "batch_size, num_queries, valid_lens = 2, 4, torch.tensor([3, 2])\n",
    "X = torch.ones((batch_size, num_queries, num_hiddens))\n",
    "output_after_attention = multi_head_attention(X, X, X, valid_lens)\n",
    "\n",
    "print(output_after_attention)\n",
    "print(output_after_attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7624b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3333, 0.3333, 0.3333, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000]],\n",
      "\n",
      "        [[0.3333, 0.3333, 0.3333, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000]],\n",
      "\n",
      "        [[0.3333, 0.3333, 0.3333, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000]],\n",
      "\n",
      "        [[0.3333, 0.3333, 0.3333, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000]],\n",
      "\n",
      "        [[0.3333, 0.3333, 0.3333, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([10, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(multi_head_attention.attention.attention_weights)\n",
    "print(multi_head_attention.attention.attention_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc823caf",
   "metadata": {},
   "source": [
    "# Reference\n",
    "* https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
