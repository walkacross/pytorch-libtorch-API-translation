{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74a7f6b0",
   "metadata": {},
   "source": [
    "# understand Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5823b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pragma cling add_include_path(\"../../libtorch/include\")\n",
    "#pragma cling add_include_path(\"../../libtorch/include/torch/csrc/api/include\")\n",
    "#pragma cling add_library_path(\"../../libtorch/lib\")\n",
    "#pragma cling load(\"libtorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f9c3b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <iostream>\n",
    "#include <tuple>\n",
    "#include <string>\n",
    "#include <vector>\n",
    "#include <memory>\n",
    "#include <functional>\n",
    "#include <type_traits>\n",
    "#include <torch/torch.h>\n",
    "#include <torch/script.h>\n",
    "namespace nn = torch::nn;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17084161",
   "metadata": {},
   "source": [
    "* Part I: how to use torch::nn::Module\n",
    "* Part II: understand it by implement it\n",
    "\n",
    "# Part I: how to use torch::nn::Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd599a1",
   "metadata": {},
   "source": [
    "## 1 Defining the Neural Network Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fdafc7",
   "metadata": {},
   "source": [
    "In line with the Python interface, neural networks based on the C++ frontend are composed of reusable building blocks called modules. There is a base module class from which all other modules are derived. In Python, this class is torch.nn.Module and in C++ it is torch::nn::Module. Besides a forward() method that implements the algorithm the module encapsulates, a module usually contains any of three kinds of sub-objects: parameters, buffers and submodules.\n",
    "    \n",
    "Parameters and buffers store state in form of tensors. Parameters record gradients, while buffers do not. Parameters are usually the trainable weights of your neural network. Examples of buffers include means and variances for batch normalization. In order to re-use particular blocks of logic and state, the PyTorch API allows modules to be nested. A nested module is termed a submodule.\n",
    "    \n",
    "Parameters, buffers and submodules must be explicitly registered. Once registered, methods like parameters() or buffers() can be used to retrieve a container of all parameters in the entire (nested) module hierarchy. Similarly, methods like to(...), where e.g. to(torch::kCUDA) moves all parameters and buffers from CPU to CUDA memory, work on the entire module hierarchy.\n",
    "\n",
    "## 1.1 Defining a Module and Registering Parameters\n",
    "To put these words into code, let’s consider this simple module written in the Python interface:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb12769c",
   "metadata": {},
   "source": [
    "~~~\n",
    "import torch\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "  def __init__(self, N, M):\n",
    "    super(Net, self).__init__()\n",
    "    self.W = torch.nn.Parameter(torch.randn(N, M))\n",
    "    self.b = torch.nn.Parameter(torch.randn(M))\n",
    "\n",
    "  def forward(self, input):\n",
    "    return torch.addmm(self.b, input, self.W)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401dbc4b",
   "metadata": {},
   "source": [
    "In C++, it would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e61d2242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWithParameter : torch::nn::Module {\n",
    "  NetWithParameter(int64_t N, int64_t M) {\n",
    "    W = register_parameter(\"W\", torch::randn({N, M}));\n",
    "    b = register_parameter(\"b\", torch::randn(M));\n",
    "  }\n",
    "  torch::Tensor forward(torch::Tensor input) {\n",
    "    return torch::addmm(b, input, W);\n",
    "  }\n",
    "  torch::Tensor W, b;\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5425a3",
   "metadata": {},
   "source": [
    "Just like in Python, we define a class called Net (for simplicity here a struct instead of a class) and derive it from the module base class. Inside the constructor, we create tensors using torch::randn just like we use torch.randn in Python. One interesting difference is how we register the parameters. In Python, we wrap the tensors with the torch.nn.Parameter class, while in C++ we have to pass the tensor through the register_parameter method instead. The reason for this is that the Python API can detect that an attribute is of type torch.nn.Parameter and automatically registers such tensors. In C++, reflection is very limited, so a more traditional (and less magical) approach is provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f0ff36",
   "metadata": {},
   "source": [
    "## 1.2 Registering Submodules and Traversing the Module Hierarchy\n",
    "\n",
    "In the same way we can register parameters, we can also register submodules. In Python, submodules are automatically detected and registered when they are assigned as an attribute of a module:\n",
    "\n",
    "~~~\n",
    "class Net(torch.nn.Module):\n",
    "  def __init__(self, N, M):\n",
    "      super(Net, self).__init__()\n",
    "      # Registered as a submodule behind the scenes\n",
    "      self.linear = torch.nn.Linear(N, M)\n",
    "      self.another_bias = torch.nn.Parameter(torch.rand(M))\n",
    "\n",
    "  def forward(self, input):\n",
    "    return self.linear(input) + self.another_bias\n",
    "~~~\n",
    "This allows, for example, to use the parameters() method to recursively access all parameters in our module hierarchy:\n",
    "~~~\n",
    ">>> net = Net(4, 5)\n",
    ">>> print(list(net.parameters()))\n",
    "[Parameter containing:\n",
    "tensor([0.0808, 0.8613, 0.2017, 0.5206, 0.5353], requires_grad=True), Parameter containing:\n",
    "tensor([[-0.3740, -0.0976, -0.4786, -0.4928],\n",
    "        [-0.1434,  0.4713,  0.1735, -0.3293],\n",
    "        [-0.3467, -0.3858,  0.1980,  0.1986],\n",
    "        [-0.1975,  0.4278, -0.1831, -0.2709],\n",
    "        [ 0.3730,  0.4307,  0.3236, -0.0629]], requires_grad=True), Parameter containing:\n",
    "tensor([ 0.2038,  0.4638, -0.2023,  0.1230, -0.0516], requires_grad=True)]\n",
    "~~~\n",
    "\n",
    "To register submodules in C++, use the aptly named register_module() method to register a module like torch::nn::Linear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f94d14a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net : public torch::nn::Module {\n",
    "    public:\n",
    "    Net(int64_t N, int64_t M):linear(register_module(\"linear\", torch::nn::Linear(N, M))) {\n",
    "    another_bias = register_parameter(\"b\", torch::randn(M));\n",
    "  }\n",
    "    \n",
    "  torch::Tensor forward(torch::Tensor input) {\n",
    "    return linear(input) + another_bias;\n",
    "  }\n",
    "    \n",
    "  torch::nn::Linear linear;\n",
    "  torch::Tensor another_bias;\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7fef03",
   "metadata": {},
   "source": [
    "One subtlety about the above code is why the submodule was created in the constructor’s initializer list, while the parameter was created inside the constructor body. There is a good reason for this, which we’ll touch upon this in the section on the C++ frontend’s ownership model further below. The end result, however, is that we can recursively access our module tree’s parameters just like in Python. Calling parameters() returns a std::vector<torch::Tensor>, which we can iterate over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e32bdab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "int main() {\n",
    "  Net net(4, 5);\n",
    "    \n",
    "  std::vector<torch::Tensor> parameters = net.parameters(); \n",
    "  for (const auto& p : parameters) {\n",
    "    std::cout << p << std::endl;\n",
    "  }\n",
    "    \n",
    "  torch::OrderedDict<std::string, torch::Tensor> ordered_parameter_dict = net.named_parameters();\n",
    "  for (const auto& pair : ordered_parameter_dict) {\n",
    "  std::cout << pair.key() << \": \" << pair.value() << std::endl;\n",
    "}\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cacde61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1.5935\n",
      "-1.1713\n",
      " 0.7192\n",
      "-0.9615\n",
      "-0.7630\n",
      "[ CPUFloatType{5} ]\n",
      "-0.0294  0.1393 -0.1203  0.1185\n",
      "-0.3770 -0.4968 -0.0325  0.1521\n",
      " 0.2261  0.2344  0.2131 -0.1853\n",
      "-0.4308  0.3387 -0.2980 -0.0603\n",
      " 0.4718 -0.2516 -0.0566 -0.3367\n",
      "[ CPUFloatType{5,4} ]\n",
      " 0.0899\n",
      " 0.2099\n",
      " 0.2911\n",
      "-0.3102\n",
      " 0.4213\n",
      "[ CPUFloatType{5} ]\n",
      "b:  1.5935\n",
      "-1.1713\n",
      " 0.7192\n",
      "-0.9615\n",
      "-0.7630\n",
      "[ CPUFloatType{5} ]\n",
      "linear.weight: -0.0294  0.1393 -0.1203  0.1185\n",
      "-0.3770 -0.4968 -0.0325  0.1521\n",
      " 0.2261  0.2344  0.2131 -0.1853\n",
      "-0.4308  0.3387 -0.2980 -0.0603\n",
      " 0.4718 -0.2516 -0.0566 -0.3367\n",
      "[ CPUFloatType{5,4} ]\n",
      "linear.bias:  0.0899\n",
      " 0.2099\n",
      " 0.2911\n",
      "-0.3102\n",
      " 0.4213\n",
      "[ CPUFloatType{5} ]\n"
     ]
    }
   ],
   "source": [
    "main();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2880fb",
   "metadata": {},
   "source": [
    "## 1.3 Running the Network in Forward Mode\n",
    "To execute the network in C++, we simply call the forward() method we defined ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ff50532",
   "metadata": {},
   "outputs": [],
   "source": [
    "int main() {\n",
    "  Net net(4, 5);\n",
    "  std::cout << net.forward(torch::ones({2, 4})) << std::endl;\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c03d8d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1.0008  1.6886 -0.4196 -1.2001 -1.4664\n",
      " 1.0008  1.6886 -0.4196 -1.2001 -1.4664\n",
      "[ CPUFloatType{2,5} ]\n"
     ]
    }
   ],
   "source": [
    "main();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dded07e5",
   "metadata": {},
   "source": [
    "# 2 Module Ownership"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52944ee",
   "metadata": {},
   "source": [
    "At this point, we know how to define a module in C++, register parameters, register submodules, traverse the module hierarchy via methods like parameters() and finally run the module’s forward() method. While there are many more methods, classes and topics to devour in the C++ API, I will refer you to docs for the full menu. We’ll also touch upon some more concepts as we implement the DCGAN model and end-to-end training pipeline in just a second. Before we do so, let me briefly touch upon the ownership model the C++ frontend provides for subclasses of torch::nn::Module.\n",
    "\n",
    "**For this discussion, the ownership model refers to the way modules are stored and passed around – which determines who or what owns a particular module instance. In Python, objects are always allocated dynamically (on the heap) and have reference semantics.** This is very easy to work with and straightforward to understand. In fact, in Python, you can largely forget about **where objects live and how they get referenced**, and focus on getting things done.\n",
    "\n",
    "C++, being a lower level language, provides more options in this realm. This increases complexity and heavily influences the design and ergonomics of the C++ frontend. In particular, for modules in the C++ frontend, **we have the option of using either value semantics or reference semantics.** The first case is the simplest and was shown in the examples thus far: **module objects are allocated on the stack and when passed to a function**, can be either copied, moved (with std::move) or taken by reference or by pointer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5867c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Net : torch::nn::Module { };\n",
    "\n",
    "void a(Net net) { }\n",
    "void b(Net& net) { }\n",
    "void c(Net* net) { }\n",
    "\n",
    "int main() {\n",
    "  Net net;\n",
    "  a(net);\n",
    "  a(std::move(net));\n",
    "  b(net);\n",
    "  c(&net);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422c253c",
   "metadata": {},
   "source": [
    "For the second case – reference semantics – we can use std::shared_ptr. The advantage of reference semantics is that, like in Python, it reduces the cognitive overhead of thinking about how modules must be passed to functions and how arguments must be declared (assuming you use shared_ptr everywhere)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5f3c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Net : torch::nn::Module {};\n",
    "\n",
    "void a(std::shared_ptr<Net> net) { }\n",
    "\n",
    "int main() {\n",
    "  auto net = std::make_shared<Net>();\n",
    "  a(net);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440ff15f",
   "metadata": {},
   "source": [
    "In our experience, researchers coming from dynamic languages greatly prefer reference semantics over value semantics, even though the latter is more “native” to C++. It is also important to note that torch::nn::Module’s design, in order to stay close to the ergonomics of the Python API, relies on shared ownership. For example, take our earlier (here shortened) definition of Net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d828a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Net : torch::nn::Module {\n",
    "  Net(int64_t N, int64_t M)\n",
    "    : linear(register_module(\"linear\", torch::nn::Linear(N, M)))\n",
    "  { }\n",
    "  torch::nn::Linear linear;\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2577cb3e",
   "metadata": {},
   "source": [
    "**in order to use the linear submodule, we want to store it directly in our class. However, we also want the module base class to know about and have access to this submodule. For this, it must store a reference to this submodule. At this point, we have already arrived at the need for shared ownership. Both the torch::nn::Module class and concrete Net class require a reference to the submodule. For this reason, the base class stores modules as shared_ptrs, and therefore the concrete class must too.**\n",
    "\n",
    "But wait! I don’t see any mention of shared_ptr in the above code! Why is that? Well, because std::shared_ptr<MyModule> is a hell of a lot to type. To keep our researchers productive, we came up with an elaborate scheme to hide the mention of shared_ptr(**the PIMPL technique**) – **a benefit usually reserved for value semantics – while retaining reference semantics.** To understand how this works, we can take a look at a simplified definition of the torch::nn::Linear module in the core library (the full definition is here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5153d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct LinearImpl : torch::nn::Module {\n",
    "  LinearImpl(int64_t in, int64_t out);\n",
    "\n",
    "  Tensor forward(const Tensor& input);\n",
    "\n",
    "  Tensor weight, bias;\n",
    "};\n",
    "\n",
    "TORCH_MODULE(Linear);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f095e4ee",
   "metadata": {},
   "source": [
    "In brief: the module is not called Linear, but LinearImpl. A macro, TORCH_MODULE then defines the actual Linear class. This “generated” class is effectively a wrapper over a std::shared_ptr<LinearImpl>. It is a wrapper instead of a simple typedef so that, among other things, constructors still work as expected, i.e. you can still write torch::nn::Linear(3, 4) instead of std::make_shared<LinearImpl>(3, 4). We call the class created by the macro the module holder. Like with (shared) pointers, you access the underlying object using the arrow operator (like model->forward(...)). The end result is an ownership model that resembles that of the Python API quite closely. **Reference semantics become the default, but without the extra typing of std::shared_ptr or std::make_shared.** For our Net, using the module holder API looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9bbd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct NetImpl : torch::nn::Module {};\n",
    "TORCH_MODULE(Net);\n",
    "\n",
    "void a(Net net) { }\n",
    "\n",
    "int main() {\n",
    "  Net net;\n",
    "  a(net);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69815449",
   "metadata": {},
   "source": [
    "There is one subtle issue that deserves mention here. A default constructed std::shared_ptr is “empty”, i.e. contains a null pointer. What is a default constructed Linear or Net? Well, it’s a tricky choice. We could say it should be an empty (null) std::shared_ptr<LinearImpl>. However, recall that Linear(3, 4) is the same as std::make_shared<LinearImpl>(3, 4). This means that if we had decided that Linear linear; should be a null pointer, then there would be no way to construct a module that does not take any constructor arguments, or defaults all of them. For this reason, in the current API, a default constructed module holder (like Linear()) invokes the default constructor of the underlying module (LinearImpl()). If the underlying module does not have a default constructor, you get a compiler error. To instead construct the empty holder, you can pass nullptr to the constructor of the holder.\n",
    "\n",
    "In practice, this means you can use submodules either like shown earlier, where the module is registered and constructed in the initializer list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f03b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Net : torch::nn::Module {\n",
    "  Net(int64_t N, int64_t M)\n",
    "    : linear(register_module(\"linear\", torch::nn::Linear(N, M)))\n",
    "  { }\n",
    "  torch::nn::Linear linear;\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32d9e10",
   "metadata": {},
   "source": [
    "or you can first construct the holder with a null pointer and then assign to it in the constructor (more familiar for Pythonistas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ce63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Net : torch::nn::Module {\n",
    "  Net(int64_t N, int64_t M) {\n",
    "    linear = register_module(\"linear\", torch::nn::Linear(N, M));\n",
    "  }\n",
    "  torch::nn::Linear linear{nullptr}; // construct an empty holder\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae37a59",
   "metadata": {},
   "source": [
    "In conclusion: Which ownership model – which semantics – should you use? The C++ frontend’s API best supports the ownership model provided by module holders. The only disadvantage of this mechanism is one extra line of boilerplate below the module declaration. That said, the simplest model is still the value semantics model shown in the introduction to C++ modules. For small, simple scripts, you may get away with it too. But you’ll find sooner or later that, for technical reasons, it is not always supported. For example, the serialization API (torch::save and torch::load) only supports module holders (or plain shared_ptr). As such, the module holder API is the recommended way of defining modules with the C++ frontend, and we will use this API in this tutorial henceforth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc0130d",
   "metadata": {},
   "source": [
    "* conclusion 1: if you define your net, like class Net: public torch:nn::Module, then use the instance of this class, you should use the value seantics.\n",
    "\n",
    "* conclusion 2: if you define your net, like class NetImpl: public torch:nn::Module, then TORCH_MODULE(Net); you can use reference seantics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746a7031",
   "metadata": {},
   "source": [
    "# reference\n",
    "\n",
    "https://pytorch.org/tutorials/advanced/cpp_frontend.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce50107c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++17",
   "language": "C++17",
   "name": "xcpp17"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
