{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed7f1b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pragma cling add_include_path(\"./libtorch/include\")\n",
    "#pragma cling add_include_path(\"./libtorch/include/torch/csrc/api/include\")\n",
    "#pragma cling add_library_path(\"./libtorch/lib\")\n",
    "#pragma cling load(\"libtorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b6d5cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <iostream>\n",
    "#include <tuple>\n",
    "#include <string>\n",
    "#include <vector>\n",
    "#include <memory>\n",
    "#include <type_traits>\n",
    "#include <torch/torch.h>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c1f5aa",
   "metadata": {},
   "source": [
    "# the relation between nn::Module, nn::ModuleHolder and nn::AnyModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d5442e",
   "metadata": {},
   "source": [
    "## 1.1 nn::Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5958dc9",
   "metadata": {},
   "source": [
    "~~~\n",
    "class TORCH_API Module : public std::enable_shared_from_this<Module> {\n",
    " public:\n",
    "  /// Returns the parameters of this `Module` and if `recurse` is true, also\n",
    "  /// recursively of every submodule.\n",
    "  std::vector<Tensor> parameters(bool recurse = true) const;\n",
    "\n",
    "  /// Returns an `OrderedDict` with the parameters of this `Module` along with\n",
    "  /// their keys, and if `recurse` is true also recursively of every submodule.\n",
    "  OrderedDict<std::string, Tensor> named_parameters(bool recurse = true) const;\n",
    "  \n",
    "  /// Registers a submodule with this `Module`.\n",
    "  ///\n",
    "  /// Registering a module makes it available to methods such as `modules()`,\n",
    "  /// `clone()` or `to()`.\n",
    "  ///\n",
    "  /// \\rst\n",
    "  /// .. code-block:: cpp\n",
    "  ///\n",
    "  ///   MyModule::MyModule() {\n",
    "  ///     submodule_ = register_module(\"linear\", torch::nn::Linear(3, 4));\n",
    "  ///   }\n",
    "  /// \\endrst\n",
    "  template <typename ModuleType>\n",
    "  std::shared_ptr<ModuleType> register_module(\n",
    "      std::string name,\n",
    "      std::shared_ptr<ModuleType> module);\n",
    "\n",
    "  /// Registers a submodule with this `Module`.\n",
    "  ///\n",
    "  /// This method deals with `ModuleHolder`s.\n",
    "  ///\n",
    "  /// Registering a module makes it available to methods such as `modules()`,\n",
    "  /// `clone()` or `to()`.\n",
    "  ///\n",
    "  /// \\rst\n",
    "  /// .. code-block:: cpp\n",
    "  ///\n",
    "  ///   MyModule::MyModule() {\n",
    "  ///     submodule_ = register_module(\"linear\", torch::nn::Linear(3, 4));\n",
    "  ///   }\n",
    "  /// \\endrst\n",
    "  template <typename ModuleType>\n",
    "  std::shared_ptr<ModuleType> register_module(\n",
    "      std::string name,\n",
    "      ModuleHolder<ModuleType> module_holder);\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "013ee677",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetImpl : public torch::nn::Module {\n",
    "    public:\n",
    "    NetImpl(int64_t N, int64_t M) {\n",
    "        W = register_parameter(\"W\", torch::randn({N, M}));\n",
    "        b = register_parameter(\"b\", torch::randn(M));\n",
    "      }\n",
    "      torch::Tensor forward(torch::Tensor input) {\n",
    "            return torch::addmm(b, input, W);\n",
    "      }\n",
    "      torch::Tensor W, b;\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6c5eb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "NetImpl model_impl{2,3};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f78bbbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.5087  1.5692  0.2568\n",
      " 0.5782  0.2423 -1.9050\n",
      "-0.7325  1.1485 -0.5545\n",
      "-4.7999  2.3417  3.7989\n",
      "[ CPUFloatType{4,3} ]\n"
     ]
    }
   ],
   "source": [
    "torch::Tensor m = torch::randn({4,2});\n",
    "torch::Tensor out = model_impl.forward(m);\n",
    "std::cout << out << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c82d02",
   "metadata": {},
   "source": [
    "## 1.2 nn::ModuleHolder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eb1e35",
   "metadata": {},
   "source": [
    "~~~\n",
    "// https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/nn/pimpl.h\n",
    "\n",
    "template <typename Contained>\n",
    "class ModuleHolder : torch::detail::ModuleHolderIndicator {\n",
    " protected:\n",
    "  /// The module pointer this class wraps.\n",
    "  /// NOTE: Must be placed at the top of the class so that we can use it with\n",
    "  /// trailing return types below.\n",
    "  // NOLINTNEXTLINE(cppcoreguidelines-non-private-member-variables-in-classes)\n",
    "  std::shared_ptr<Contained> impl_;\n",
    "\n",
    " public:\n",
    "  using ContainedType = Contained;\n",
    "\n",
    "  /// Default constructs the contained module if if has a default constructor,\n",
    "  /// else produces a static error.\n",
    "  ///\n",
    "  /// NOTE: This uses the behavior of template\n",
    "  /// classes in C++ that constructors (or any methods) are only compiled when\n",
    "  /// actually used.\n",
    "  ModuleHolder() : impl_(default_construct()) {\n",
    "    static_assert(\n",
    "        std::is_default_constructible<Contained>::value,\n",
    "        \"You are trying to default construct a module which has \"\n",
    "        \"no default constructor. Use = nullptr to give it the empty state \"\n",
    "        \"(e.g. `Linear linear = nullptr;` instead of `Linear linear;`).\");\n",
    "  }\n",
    "\n",
    "  /// Constructs the `ModuleHolder` with an empty contained value. Access to\n",
    "  /// the underlying module is not permitted and will throw an exception, until\n",
    "  /// a value is assigned.\n",
    "  /* implicit */ ModuleHolder(std::nullptr_t) : impl_(nullptr) {}\n",
    "  \n",
    "  /// Calls the `forward()` method of the contained module.\n",
    "  template <typename... Args>\n",
    "  auto operator()(Args&&... args)\n",
    "      -> torch::detail::return_type_of_forward_t<Contained, Args...> {\n",
    "    // This will not compile if the module does not have a `forward()` method\n",
    "    // (as expected).\n",
    "    // NOTE: `std::forward` is qualified to prevent VS2017 emitting\n",
    "    // error C2872: 'std': ambiguous symbol\n",
    "    return impl_->forward(::std::forward<Args>(args)...);\n",
    "  }\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91a5659",
   "metadata": {},
   "source": [
    "~~~\n",
    "#define TORCH_MODULE_IMPL(Name, ImplType)                              \\\n",
    "  class Name : public torch::nn::ModuleHolder<ImplType> { /* NOLINT */ \\\n",
    "   public:                                                             \\\n",
    "    using torch::nn::ModuleHolder<ImplType>::ModuleHolder;             \\\n",
    "    using Impl TORCH_UNUSED_EXCEPT_CUDA = ImplType;                    \\\n",
    "  }\n",
    "\n",
    "/// Like `TORCH_MODULE_IMPL`, but defaults the `ImplType` name to `<Name>Impl`.\n",
    "#define TORCH_MODULE(Name) TORCH_MODULE_IMPL(Name, Name##Impl)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd989574",
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCH_MODULE(Net);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f80d9e",
   "metadata": {},
   "source": [
    "## 1.3 nn::AnyModulenn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59840306",
   "metadata": {},
   "source": [
    "~~~\n",
    "//https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/nn/modules/container/any.h\n",
    "\n",
    "///   torch::nn::AnyModule module(torch::nn::Linear(3, 4));\n",
    "///   std::shared_ptr<nn::Module> ptr = module.ptr();\n",
    "///   torch::nn::Linear linear(module.get<torch::nn::Linear>());\n",
    "/// \\endrst\n",
    "class AnyModule {\n",
    " public:\n",
    " \n",
    " /// The type erased module.\n",
    "  std::unique_ptr<AnyModulePlaceholder> content_;\n",
    "  \n",
    "  \n",
    "  /// A default-constructed `AnyModule` is in an empty state.\n",
    "  AnyModule() = default;\n",
    "\n",
    "  /// Constructs an `AnyModule` from a `shared_ptr` to concrete module object.\n",
    "  template <typename ModuleType>\n",
    "  explicit AnyModule(std::shared_ptr<ModuleType> module);\n",
    "\n",
    "  /// Constructs an `AnyModule` from a concrete module object.\n",
    "  template <\n",
    "      typename ModuleType,\n",
    "      typename = torch::detail::enable_if_module_t<ModuleType>>\n",
    "  explicit AnyModule(ModuleType&& module);\n",
    "\n",
    "  /// Constructs an `AnyModule` from a module holder.\n",
    "  template <typename ModuleType>\n",
    "  explicit AnyModule(const ModuleHolder<ModuleType>& module_holder);\n",
    "\n",
    "\n",
    "template <typename... ArgumentTypes>\n",
    "AnyValue AnyModule::any_forward(ArgumentTypes&&... arguments) {\n",
    "  TORCH_CHECK(!is_empty(), \"Cannot call forward() on an empty AnyModule\");\n",
    "  std::vector<AnyValue> values;\n",
    "  values.reserve(sizeof...(ArgumentTypes));\n",
    "  torch::apply(\n",
    "      [&values](AnyValue&& value) { values.push_back(std::move(value)); },\n",
    "      AnyValue(std::forward<ArgumentTypes>(arguments))...);\n",
    "  return content_->forward(std::move(values));\n",
    "}\n",
    "\n",
    "template <typename ReturnType, typename... ArgumentTypes>\n",
    "ReturnType AnyModule::forward(ArgumentTypes&&... arguments) {\n",
    "  return any_forward(std::forward<ArgumentTypes>(arguments)...)\n",
    "      .template get<ReturnType>();\n",
    "}\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ab26db",
   "metadata": {},
   "source": [
    "### a construct from ModuleHolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "607e74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch::nn::AnyModule module(torch::nn::Linear(2, 3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7d4fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch::Tensor x = torch::randn({4,2});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd19a19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1699  0.9128  0.3406\n",
      " 1.3906 -0.3705  0.4861\n",
      "-0.1416  1.3699  0.2101\n",
      " 0.9539 -0.7656  0.6544\n",
      "[ CPUFloatType{4,3} ]\n"
     ]
    }
   ],
   "source": [
    "torch::Tensor output = module.forward(x);\n",
    "std::cout << output << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ee6ebb",
   "metadata": {},
   "source": [
    "## b construct from std::shared_ptr point to nn::Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7c6de90",
   "metadata": {},
   "outputs": [],
   "source": [
    "std::shared_ptr<torch::nn::LinearImpl> module_ptr(std::make_shared<torch::nn::LinearImpl>(2,3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "493eac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch::nn::AnyModule module_b(module_ptr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "491a8f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.7532 -0.1333  0.0107\n",
      " 0.7393 -0.1115  0.3020\n",
      " 1.0847 -0.4567 -0.0311\n",
      " 0.9058 -0.2797  0.0773\n",
      "[ CPUFloatType{4,3} ]\n"
     ]
    }
   ],
   "source": [
    "torch::Tensor x2 = torch::randn({4,2});\n",
    "torch::Tensor output2 = module_b.forward(x2);\n",
    "std::cout << output2 << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05ed8d2",
   "metadata": {},
   "source": [
    "# 2 how to perform polymorphically for nn::Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3150a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch::Tensor model_impl_process(torch::nn::AnyModule model){\n",
    "    torch::Tensor x = torch::randn({4,2});\n",
    "    torch::Tensor output = model.forward(x);\n",
    "    std::cout << output << std::endl;\n",
    "    return output;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c41354c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.7436  0.6978  0.2771\n",
      " 0.1955  1.3015  0.1836\n",
      " 0.6660 -0.1387  0.5194\n",
      " 0.9377 -1.0845  0.7449\n",
      "[ CPUFloatType{4,3} ]\n"
     ]
    }
   ],
   "source": [
    "//torch::Tensor a = model_impl_process(model_impl);\n",
    "torch::Tensor a = model_impl_process(module);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e4134cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.2503  0.3578  0.0910\n",
      "-0.0873  0.6288 -1.9189\n",
      "-0.5216  1.0904 -0.5350\n",
      " 1.2251 -0.5861  0.2206\n",
      "[ CPUFloatType{4,3} ]\n"
     ]
    }
   ],
   "source": [
    "torch::Tensor b = model_impl_process(module_b);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cffd916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.9709 -1.7128  2.0667\n",
      " 1.1812 -2.0922  0.4920\n",
      "-1.5993 -0.4468  1.3714\n",
      " 2.4313 -1.3531 -0.4786\n",
      "[ CPUFloatType{4,3} ]\n"
     ]
    }
   ],
   "source": [
    "torch::nn::AnyModule net_any_module(Net{2,3});\n",
    "torch::Tensor c = model_impl_process(net_any_module);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++14",
   "language": "C++14",
   "name": "xcpp14"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
