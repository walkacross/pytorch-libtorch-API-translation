{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1366a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32d9488",
   "metadata": {},
   "source": [
    "# 1.1 torch.nn.MultiheadAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105ee6f0",
   "metadata": {},
   "source": [
    "## a: core usage\n",
    "\n",
    "~~~\n",
    "forward(query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None, average_attn_weights=True)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3b7a17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attn = torch.nn.MultiheadAttention(embed_dim=4, num_heads=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4274b0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4028, -0.8381, -0.9777, -0.3953],\n",
      "         [-0.3942,  0.4389,  1.2422, -0.9700],\n",
      "         [-1.1927,  1.3185, -1.0513, -1.5547]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((1,3,4))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "def2aefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output, attn_output_weights = multihead_attn(x,x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "954df47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0881,  0.2321, -0.0496, -0.1493],\n",
      "         [ 0.1222,  0.2689, -0.1273, -0.2522],\n",
      "         [ 0.2165,  0.3881, -0.3285, -0.5227]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ad56d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3571, 0.3111, 0.3318],\n",
      "         [0.2507, 0.3507, 0.3986],\n",
      "         [0.1129, 0.2516, 0.6355]]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attn_output_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f86428e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3745,  0.2747, -0.3115, -0.9587],\n",
       "         [-0.5127,  0.4693, -0.2285, -1.0590],\n",
       "         [-0.8116,  0.8536, -0.4660, -1.2767]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output_weights.matmul(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52ca613",
   "metadata": {},
   "source": [
    "## b understand attn_mask in multihead_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884f5f00",
   "metadata": {},
   "source": [
    "* attn_mask – If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape (L, S)(L,S) or (N\\cdot\\text{num\\_heads}, L, S)(N⋅num_heads,L,S), where NN is the batch size, LL is the target sequence length, and SS is the source sequence length. A 2D mask will be broadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch. Binary, byte, and float masks are supported. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902f80fc",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py\n",
    "~~~\n",
    "def _scaled_dot_product_attention(\n",
    "    q: Tensor,\n",
    "    k: Tensor,\n",
    "    v: Tensor,\n",
    "    attn_mask: Optional[Tensor] = None,\n",
    "    dropout_p: float = 0.0,\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    r\"\"\"\n",
    "    Computes scaled dot product attention on query, key and value tensors, using\n",
    "    an optional attention mask if passed, and applying dropout if a probability\n",
    "    greater than 0.0 is specified.\n",
    "    Returns a tensor pair containing attended values and attention weights.\n",
    "    Args:\n",
    "        q, k, v: query, key and value tensors. See Shape section for shape details.\n",
    "        attn_mask: optional tensor containing mask values to be added to calculated\n",
    "            attention. May be 2D or 3D; see Shape section for details.\n",
    "        dropout_p: dropout probability. If greater than 0.0, dropout is applied.\n",
    "    Shape:\n",
    "        - q: :math:`(B, Nt, E)` where B is batch size, Nt is the target sequence length,\n",
    "            and E is embedding dimension.\n",
    "        - key: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n",
    "            and E is embedding dimension.\n",
    "        - value: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n",
    "            and E is embedding dimension.\n",
    "        - attn_mask: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n",
    "            shape :math:`(Nt, Ns)`.\n",
    "        - Output: attention values have shape :math:`(B, Nt, E)`; attention weights\n",
    "            have shape :math:`(B, Nt, Ns)`\n",
    "    \"\"\"\n",
    "    B, Nt, E = q.shape\n",
    "    q = q / math.sqrt(E)\n",
    "    # (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\n",
    "    if attn_mask is not None:\n",
    "        attn = torch.baddbmm(attn_mask, q, k.transpose(-2, -1))\n",
    "    else:\n",
    "        attn = torch.bmm(q, k.transpose(-2, -1))\n",
    "\n",
    "    attn = softmax(attn, dim=-1)\n",
    "    if dropout_p > 0.0:\n",
    "        attn = dropout(attn, p=dropout_p)\n",
    "    # (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)\n",
    "    output = torch.bmm(attn, v)\n",
    "    return output, attn\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80322e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf],\n",
      "        [0., 0., -inf],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[False,  True,  True],\n",
      "        [False, False,  True],\n",
      "        [False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "attn_mask = nn.Transformer.generate_square_subsequent_mask(3)\n",
    "print(attn_mask)\n",
    "attn_mask = attn_mask.bool()\n",
    "print(attn_mask)\n",
    "#attn_mask = torch.randint(0,2,[3,3]).bool()\n",
    "#print(attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baedb874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0901,  0.0645,  0.0768,  0.0714],\n",
      "         [ 0.1172,  0.4016,  0.0096,  0.0223],\n",
      "         [-0.0686,  0.2491, -0.1330, -0.1129]]], grad_fn=<TransposeBackward0>)\n",
      "tensor([[[1.0000, 0.0000, 0.0000],\n",
      "         [0.4010, 0.5990, 0.0000],\n",
      "         [0.1628, 0.1985, 0.6386]]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_output, attn_output_weights = multihead_attn(x,x,x, attn_mask=attn_mask)\n",
    "print(attn_output)\n",
    "print(attn_output_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9311ed1",
   "metadata": {},
   "source": [
    "## c: understand key_padding_mask in multihead_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb15200",
   "metadata": {},
   "source": [
    "* key_padding_mask – If specified, a mask of shape (N, S)(N,S) indicating which elements within key to ignore for the purpose of attention (i.e. treat as “padding”). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7c1ee9",
   "metadata": {},
   "source": [
    "batch_size = 3，seq_length_ =4，token looks like\n",
    "~~~\n",
    "[\n",
    "    [‘a’,'b','<PAD>'],\n",
    "    [‘a’,'b','c'],\n",
    "    [‘a’,'<PAD>','<PAD>']\n",
    "]\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e105e7",
   "metadata": {},
   "source": [
    "key_padding_mask.shape = （3,4）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f772e9",
   "metadata": {},
   "source": [
    "~~~\n",
    "padding_mask = torch.tensor([\n",
    "    [False, False, True],\n",
    "    [False, False, False],\n",
    "    [False, True, True]\n",
    "])\n",
    "print(padding_mask)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "639cb8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask = torch.tensor([\n",
    "    [False, False, True]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8c8c970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00bc1369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1128, 0.3470, 0.0205, 0.0303],\n",
      "         [0.1172, 0.4016, 0.0096, 0.0223],\n",
      "         [0.1149, 0.3737, 0.0152, 0.0264]]], grad_fn=<TransposeBackward0>)\n",
      "tensor([[[0.4980, 0.5020, 0.0000],\n",
      "         [0.4010, 0.5990, 0.0000],\n",
      "         [0.4506, 0.5494, 0.0000]]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_output, attn_output_weights = multihead_attn(x,x,x, key_padding_mask=padding_mask)\n",
    "print(attn_output)\n",
    "print(attn_output_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c91b1a9",
   "metadata": {},
   "source": [
    "# d: mix use of attn_mask and padding mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be5b95a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0901, 0.0645, 0.0768, 0.0714],\n",
      "         [0.1172, 0.4016, 0.0096, 0.0223],\n",
      "         [0.1149, 0.3737, 0.0152, 0.0264]]], grad_fn=<TransposeBackward0>)\n",
      "tensor([[[1.0000, 0.0000, 0.0000],\n",
      "         [0.4010, 0.5990, 0.0000],\n",
      "         [0.4506, 0.5494, 0.0000]]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_output, attn_output_weights = multihead_attn(x,x,x, attn_mask=attn_mask, key_padding_mask=padding_mask)\n",
    "print(attn_output)\n",
    "print(attn_output_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e07004",
   "metadata": {},
   "source": [
    "# 1.2 nn.TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b8ba37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=4, nhead=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf3085f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.8674, -1.5524,  0.4267, -0.1954],\n",
      "         [-2.1481, -0.5348, -1.4551, -0.3789],\n",
      "         [-1.9665,  0.4252, -0.1385, -0.3649]]])\n"
     ]
    }
   ],
   "source": [
    "src = torch.randn(1,3,4)\n",
    "print(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0479161c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.9521, -1.6815,  0.3756,  0.3538],\n",
      "         [-0.9838,  0.6514, -0.9635,  1.2959],\n",
      "         [-1.5548,  1.0577, -0.1639,  0.6610]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = encoder_layer(src)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c4e07f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8387e326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True, False],\n",
      "        [False,  True, False],\n",
      "        [False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "attn_mask = torch.randint(0,2,[3,3]).bool()\n",
    "print(attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd2dbb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.7793, -1.7167,  0.5236,  0.4138],\n",
      "         [-0.9969,  1.1466, -0.9914,  0.8417],\n",
      "         [-1.5465,  1.1288, -0.1426,  0.5603]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = encoder_layer(src, src_mask=attn_mask)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be50c0ec",
   "metadata": {},
   "source": [
    "# 1.3 torch.nn.TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95cd08aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d0ecda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.0504, -1.6393,  0.4283,  0.1606],\n",
      "         [-0.7235,  0.7281, -1.2160,  1.2115],\n",
      "         [-1.2552,  1.0471, -0.7041,  0.9122]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = transformer_encoder(src)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2a9dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
