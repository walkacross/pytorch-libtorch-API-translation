{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1366a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sz/anaconda3/envs/cling/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212f3c6d",
   "metadata": {},
   "source": [
    "a clean and readable version to understand the notation in transformer\n",
    "\n",
    "https://github.com/pytorch/pytorch/blob/4bf90558e0cbafbf03fa7e4285367f12658bde54/torch/nn/modules/transformer.py#L296"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32d9488",
   "metadata": {},
   "source": [
    "# 1.1 torch.nn.MultiheadAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105ee6f0",
   "metadata": {},
   "source": [
    "## a: core usage\n",
    "\n",
    "~~~\n",
    "forward(query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None, average_attn_weights=True)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3b7a17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attn = torch.nn.MultiheadAttention(embed_dim=5, num_heads=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b24420e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 1\n",
    "# sequence_length = 4\n",
    "# embedding_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4274b0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2691, -0.0816,  0.9442,  0.9448, -0.1601],\n",
      "         [ 0.3016, -0.2399,  0.7206, -0.4855, -1.1777],\n",
      "         [-0.5336,  0.1804,  0.3119,  1.3854, -0.2336],\n",
      "         [ 1.3802,  0.3300, -0.5571,  0.8665,  1.1283]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((1,4,5))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "def2aefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output, attn_output_weights = multihead_attn(x,x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "954df47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0664,  0.0354, -0.1694,  0.0943, -0.1532],\n",
      "         [ 0.0416,  0.0203, -0.1636,  0.0986, -0.1422],\n",
      "         [ 0.0560,  0.0305, -0.1760,  0.0884, -0.1525],\n",
      "         [ 0.0432,  0.0319, -0.1664,  0.0943, -0.1445]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "torch.Size([1, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "print(attn_output)\n",
    "print(attn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ad56d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2333, 0.2215, 0.2412, 0.3040],\n",
      "         [0.2337, 0.3121, 0.2497, 0.2045],\n",
      "         [0.2793, 0.2419, 0.2438, 0.2350],\n",
      "         [0.3042, 0.2723, 0.2123, 0.2112]]], grad_fn=<DivBackward0>)\n",
      "torch.Size([1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(attn_output_weights)\n",
    "print(attn_output_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f86428e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2949,  0.0717,  0.2857,  0.7105, -0.0115],\n",
       "         [ 0.1802,  0.0186,  0.4095,  0.5924, -0.2326],\n",
       "         [ 0.1921,  0.0407,  0.3831,  0.6878, -0.1213],\n",
       "         [ 0.1785,  0.0179,  0.4319,  0.6323, -0.1806]]],\n",
       "       grad_fn=<BmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output_weights.bmm(x) #??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52ca613",
   "metadata": {},
   "source": [
    "## b understand attn_mask in multihead_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884f5f00",
   "metadata": {},
   "source": [
    "* attn_mask – If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape (L, S)(L,S) or (N\\cdot\\text{num\\_heads}, L, S)(N⋅num_heads,L,S), where NN is the batch size, LL is the target sequence length, and SS is the source sequence length. A 2D mask will be broadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch. Binary, byte, and float masks are supported. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902f80fc",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py\n",
    "~~~\n",
    "def _scaled_dot_product_attention(\n",
    "    q: Tensor,\n",
    "    k: Tensor,\n",
    "    v: Tensor,\n",
    "    attn_mask: Optional[Tensor] = None,\n",
    "    dropout_p: float = 0.0,\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    r\"\"\"\n",
    "    Computes scaled dot product attention on query, key and value tensors, using\n",
    "    an optional attention mask if passed, and applying dropout if a probability\n",
    "    greater than 0.0 is specified.\n",
    "    Returns a tensor pair containing attended values and attention weights.\n",
    "    Args:\n",
    "        q, k, v: query, key and value tensors. See Shape section for shape details.\n",
    "        attn_mask: optional tensor containing mask values to be added to calculated\n",
    "            attention. May be 2D or 3D; see Shape section for details.\n",
    "        dropout_p: dropout probability. If greater than 0.0, dropout is applied.\n",
    "    Shape:\n",
    "        - q: :math:`(B, Nt, E)` where B is batch size, Nt is the target sequence length,\n",
    "            and E is embedding dimension.\n",
    "        - key: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n",
    "            and E is embedding dimension.\n",
    "        - value: :math:`(B, Ns, E)` where B is batch size, Ns is the source sequence length,\n",
    "            and E is embedding dimension.\n",
    "        - attn_mask: either a 3D tensor of shape :math:`(B, Nt, Ns)` or a 2D tensor of\n",
    "            shape :math:`(Nt, Ns)`.\n",
    "        - Output: attention values have shape :math:`(B, Nt, E)`; attention weights\n",
    "            have shape :math:`(B, Nt, Ns)`\n",
    "    \"\"\"\n",
    "    B, Nt, E = q.shape\n",
    "    q = q / math.sqrt(E)\n",
    "    # (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\n",
    "    if attn_mask is not None:\n",
    "        attn = torch.baddbmm(attn_mask, q, k.transpose(-2, -1))\n",
    "    else:\n",
    "        attn = torch.bmm(q, k.transpose(-2, -1))\n",
    "\n",
    "    attn = softmax(attn, dim=-1)\n",
    "    if dropout_p > 0.0:\n",
    "        attn = dropout(attn, p=dropout_p)\n",
    "    # (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)\n",
    "    output = torch.bmm(attn, v)\n",
    "    return output, attn\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb8421dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subsequent_mask(tgt_sz, src_sz): \n",
    "    mask = (torch.triu(torch.ones(src_sz, tgt_sz)) == 1).transpose(0, 1) \n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)) \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80322e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf],\n",
      "        [0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "attn_mask = generate_subsequent_mask(4,4)\n",
    "print(attn_mask)\n",
    "attn_mask = attn_mask.bool()\n",
    "print(attn_mask)\n",
    "#attn_mask = torch.randint(0,2,[3,3]).bool()\n",
    "#print(attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baedb874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0339,  0.0954, -0.2361,  0.0237, -0.1749],\n",
      "         [-0.0650,  0.0281, -0.0852,  0.1471, -0.0764],\n",
      "         [ 0.0305,  0.0017, -0.2087,  0.0633, -0.1567],\n",
      "         [ 0.0432,  0.0319, -0.1664,  0.0943, -0.1445]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4281, 0.5719, 0.0000, 0.0000],\n",
      "         [0.3651, 0.3162, 0.3187, 0.0000],\n",
      "         [0.3042, 0.2723, 0.2123, 0.2112]]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_output, attn_output_weights = multihead_attn(x,x,x, attn_mask=attn_mask)\n",
    "print(attn_output)\n",
    "print(attn_output_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "155f9b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#attn_output_weights.bmm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9311ed1",
   "metadata": {},
   "source": [
    "## c: understand key_padding_mask in multihead_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb15200",
   "metadata": {},
   "source": [
    "* key_padding_mask – If specified, a mask of shape (N, S)(N,S) indicating which elements within key to ignore for the purpose of attention (i.e. treat as “padding”). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7c1ee9",
   "metadata": {},
   "source": [
    "batch_size = 3，seq_length_ =4，token looks like\n",
    "~~~\n",
    "[\n",
    "    [‘a’,'b','c','<PAD>'],\n",
    "    [‘a’,'b','c','d'],\n",
    "    [‘a’,'b','<PAD>','<PAD>']\n",
    "]\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e105e7",
   "metadata": {},
   "source": [
    "key_padding_mask.shape = （3,4）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f772e9",
   "metadata": {},
   "source": [
    "~~~\n",
    "padding_mask = torch.tensor([\n",
    "    [False, False, False, True],\n",
    "    [False, False, False, False],\n",
    "    [False, False, True, True]\n",
    "])\n",
    "print(padding_mask)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "639cb8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask = torch.tensor([\n",
    "    [False, False, True,True]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8c8c970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00bc1369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0503,  0.0381, -0.1076,  0.1288, -0.0910],\n",
      "         [-0.0650,  0.0281, -0.0852,  0.1471, -0.0764],\n",
      "         [-0.0464,  0.0408, -0.1137,  0.1238, -0.0950],\n",
      "         [-0.0478,  0.0398, -0.1115,  0.1256, -0.0936]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[0.5129, 0.4871, 0.0000, 0.0000],\n",
      "         [0.4281, 0.5719, 0.0000, 0.0000],\n",
      "         [0.5359, 0.4641, 0.0000, 0.0000],\n",
      "         [0.5276, 0.4724, 0.0000, 0.0000]]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_output, attn_output_weights = multihead_attn(x,x,x, key_padding_mask=padding_mask)\n",
    "print(attn_output)\n",
    "print(attn_output_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c91b1a9",
   "metadata": {},
   "source": [
    "# d: mix use of attn_mask and padding mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be5b95a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0339,  0.0954, -0.2361,  0.0237, -0.1749],\n",
      "         [-0.0650,  0.0281, -0.0852,  0.1471, -0.0764],\n",
      "         [-0.0464,  0.0408, -0.1137,  0.1238, -0.0950],\n",
      "         [-0.0478,  0.0398, -0.1115,  0.1256, -0.0936]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4281, 0.5719, 0.0000, 0.0000],\n",
      "         [0.5359, 0.4641, 0.0000, 0.0000],\n",
      "         [0.5276, 0.4724, 0.0000, 0.0000]]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_output, attn_output_weights = multihead_attn(x,x,x, attn_mask=attn_mask, key_padding_mask=padding_mask)\n",
    "print(attn_output)\n",
    "print(attn_output_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e07004",
   "metadata": {},
   "source": [
    "# 1.2 nn.TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455a0feb",
   "metadata": {},
   "source": [
    "~~~\n",
    "class TransformerEncoderLayer(Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\", layer_norm_eps=1e-5):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        \n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "        \"\"\"\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc4cdb3",
   "metadata": {},
   "source": [
    "Based on the PyTorch implementation source code **src_mask** is what is called **attn_mask** in a MultiheadAttention module and **src_key_padding_mask** is equivalent to **key_padding_mask** in a MultiheadAttention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b8ba37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=5, nhead=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ef3a2",
   "metadata": {},
   "source": [
    "~~~\n",
    "forward(src, src_mask=None, src_key_padding_mask=None, is_causal=False)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf3085f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.4174,  0.3715,  0.8087,  0.5961, -1.0753],\n",
      "         [ 0.2875,  0.9670,  0.2216,  1.1212,  0.2438],\n",
      "         [-0.9028, -1.3876,  0.4233, -1.0591, -0.5709],\n",
      "         [ 0.3651,  1.9209,  0.3671, -1.0275, -0.5568]]])\n"
     ]
    }
   ],
   "source": [
    "src = torch.randn(1,4,5)\n",
    "print(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0479161c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.2363,  0.9163,  0.5926,  0.9229, -1.1955],\n",
      "         [-0.4635,  1.1635, -0.9957,  1.2424, -0.9467],\n",
      "         [-0.3861, -1.3372,  1.7499, -0.0021, -0.0246],\n",
      "         [-0.1120,  1.8660, -0.0859, -1.0653, -0.6028]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([1, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "out = encoder_layer(src)\n",
    "print(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd2dbb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1862,  0.8848,  0.5392,  0.9972, -1.2349],\n",
      "         [-0.3258,  1.0354, -1.1770,  1.3124, -0.8450],\n",
      "         [-0.0124, -1.5519,  1.5700, -0.2541,  0.2485],\n",
      "         [ 0.1095,  1.8375, -0.3342, -1.1221, -0.4907]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = encoder_layer(src, src_mask=attn_mask, src_key_padding_mask=padding_mask)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be50c0ec",
   "metadata": {},
   "source": [
    "# 1.3 torch.nn.TransformerEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124ec9f8",
   "metadata": {},
   "source": [
    "~~~\n",
    "class TransformerEncoder(Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n",
    "        num_layers: the number of sub-encoder-layers in the encoder (required).\n",
    "        norm: the layer normalization component (optional).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layers in turn.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder (required).\n",
    "            mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        output = src\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95cd08aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b8930",
   "metadata": {},
   "source": [
    "~~~\n",
    "forward(src, mask=None, src_key_padding_mask=None, is_causal=None)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d0ecda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8970,  1.1202,  0.1338,  1.0171, -1.3741],\n",
      "         [-0.3626,  1.1703, -1.1551,  1.1995, -0.8520],\n",
      "         [ 0.2637, -1.7185,  1.3872,  0.1926, -0.1250],\n",
      "         [-0.1441,  1.9542, -0.4394, -0.5676, -0.8032]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = transformer_encoder.forward(src)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6759122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5272,  1.0591,  0.0050,  1.0461, -1.5830],\n",
      "         [ 0.4179,  0.6345, -1.7489,  1.0971, -0.4006],\n",
      "         [ 0.6252, -1.7875,  1.1223, -0.2571,  0.2970],\n",
      "         [ 0.7973,  1.4865, -1.0211, -1.0280, -0.2348]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = transformer_encoder.forward(src, mask=attn_mask, src_key_padding_mask=padding_mask)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbac540f",
   "metadata": {},
   "source": [
    "## 1.4 torch.nn.TransformerDecoderLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de8a081",
   "metadata": {},
   "source": [
    "![](https://pytorch.org/tutorials/_images/seq2seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3ea097",
   "metadata": {},
   "source": [
    "~~~\n",
    "class TransformerDecoderLayer(Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\", layer_norm_eps=1e-5):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "    \n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the inputs (and mask) through the decoder layer.\n",
    "\n",
    "        Args:\n",
    "            tgt: the sequence to the decoder layer (required).\n",
    "            memory: the sequence from the last layer of the encoder (required).\n",
    "            tgt_mask: the mask for the tgt sequence (optional).\n",
    "            memory_mask: the mask for the memory sequence (optional).\n",
    "            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n",
    "            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n",
    "        \"\"\"\n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
    "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a9b0d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(d_model=5, nhead=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7551169f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5490, 0.0093, 0.4613, 0.8061, 0.5491],\n",
      "         [0.7317, 0.2648, 0.1612, 0.0341, 0.6122],\n",
      "         [0.2431, 0.7019, 0.5981, 0.7685, 0.7452],\n",
      "         [0.6860, 0.6298, 0.2847, 0.3592, 0.7690]]])\n",
      "torch.Size([1, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 1\n",
    "# sequence_length = 4\n",
    "# embedding_size = 5\n",
    "memory = torch.rand(1, 4, 5)\n",
    "print(memory)\n",
    "print(memory.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7c06b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7859, 0.2923, 0.8150, 0.3474, 0.6866],\n",
      "         [0.7447, 0.6915, 0.9891, 0.1155, 0.9180],\n",
      "         [0.7315, 0.1757, 0.1024, 0.7563, 0.4602],\n",
      "         [0.3719, 0.6723, 0.1913, 0.7401, 0.6185],\n",
      "         [0.7635, 0.4613, 0.5866, 0.3904, 0.1410],\n",
      "         [0.0569, 0.6474, 0.7203, 0.5985, 0.1828]]])\n",
      "torch.Size([1, 6, 5])\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 1\n",
    "# sequence_length = 6\n",
    "# embedding_size = 5\n",
    "tgt = torch.rand(1, 6, 5)\n",
    "print(tgt)\n",
    "print(tgt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00eefe70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.6976, -1.5439,  0.7686, -0.8375,  0.9152],\n",
      "         [-0.0777, -0.1555,  0.1359, -1.5240,  1.6213],\n",
      "         [ 0.0332, -0.9220, -1.1599,  1.6181,  0.4306],\n",
      "         [-0.6357,  0.4516, -1.6471,  0.8808,  0.9504],\n",
      "         [ 1.8324,  0.2090, -0.4004, -0.6065, -1.0346],\n",
      "         [-1.0780,  1.2596,  0.6040,  0.4954, -1.2810]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([1, 6, 5])\n"
     ]
    }
   ],
   "source": [
    "out = decoder_layer.forward(tgt, memory)\n",
    "print(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab07fe3",
   "metadata": {},
   "source": [
    "### A tgt_mask and tgt_key_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abec11ac",
   "metadata": {},
   "source": [
    "the role of tgt_mask and tgt_key_padding_mask is the same as that of in the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88825bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6])\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "tgt_mask = generate_subsequent_mask(6,6)\n",
    "print(tgt_mask.shape)\n",
    "print(tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b131b29",
   "metadata": {},
   "source": [
    "assume the last three token is <padding>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "915a4362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6])\n",
      "tensor([[False, False, False,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "tgt_padding_mask = torch.tensor([\n",
    "    [False, False, False, True,True,True]])\n",
    "print(tgt_padding_mask.shape)\n",
    "print(tgt_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99683901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4119, -1.5630,  0.3293, -0.5742,  1.3961],\n",
      "         [ 0.0660,  0.0369,  0.0420, -1.6507,  1.5058],\n",
      "         [ 0.5523, -1.0356, -1.2190,  1.4375,  0.2648],\n",
      "         [-0.7665, -0.0370, -1.4042,  1.1377,  1.0700],\n",
      "         [ 1.8948,  0.0544, -0.3442, -0.7892, -0.8157],\n",
      "         [-1.2370,  0.9065,  0.5855,  0.9385, -1.1935]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([1, 6, 5])\n"
     ]
    }
   ],
   "source": [
    "out = decoder_layer.forward(tgt, memory,tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_padding_mask)\n",
    "print(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085f9325",
   "metadata": {},
   "source": [
    "### B **pay attention to tye role of memory_mask and memory_key_padding_mask in the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f306dca",
   "metadata": {},
   "source": [
    "~~~\n",
    "self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051498b9",
   "metadata": {},
   "source": [
    "in the context of multi-head attention, the role of memory_mask and memory_key_padding_mask is to mask some position in the attention_weight to not involve in calculating the output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a442d632",
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attn = torch.nn.MultiheadAttention(embed_dim=5, num_heads=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50414d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0215, -0.2030, -0.0060, -0.1130,  0.0062],\n",
       "          [-0.0240, -0.2012, -0.0025, -0.1108,  0.0066],\n",
       "          [-0.0192, -0.2047, -0.0097, -0.1152,  0.0058],\n",
       "          [-0.0222, -0.2025, -0.0055, -0.1125,  0.0062],\n",
       "          [-0.0211, -0.2035, -0.0079, -0.1137,  0.0058],\n",
       "          [-0.0225, -0.2023, -0.0057, -0.1123,  0.0060]]],\n",
       "        grad_fn=<TransposeBackward0>),\n",
       " tensor([[[0.2664, 0.2103, 0.2888, 0.2344],\n",
       "          [0.2749, 0.2026, 0.2936, 0.2289],\n",
       "          [0.2608, 0.2196, 0.2799, 0.2397],\n",
       "          [0.2707, 0.2113, 0.2858, 0.2322],\n",
       "          [0.2708, 0.2182, 0.2752, 0.2358],\n",
       "          [0.2739, 0.2146, 0.2813, 0.2302]]], grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_attn(tgt, memory, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca7c8c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "padding_mask = torch.tensor([\n",
    "    [False, False, True,True]])\n",
    "print(padding_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f8f3102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf],\n",
      "        [0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "torch.Size([6, 4])\n"
     ]
    }
   ],
   "source": [
    "memory_mask = generate_subsequent_mask(6,4)\n",
    "print(memory_mask)\n",
    "print(memory_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0fe1dcb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.1731, -0.1094,  0.1429,  0.0015,  0.0132],\n",
       "          [-0.0675, -0.1704,  0.0132, -0.0815, -0.0047],\n",
       "          [-0.0593, -0.1752,  0.0031, -0.0879, -0.0061],\n",
       "          [-0.0640, -0.1725,  0.0089, -0.0842, -0.0053],\n",
       "          [-0.0620, -0.1736,  0.0065, -0.0858, -0.0056],\n",
       "          [-0.0638, -0.1726,  0.0086, -0.0844, -0.0054]]],\n",
       "        grad_fn=<TransposeBackward0>),\n",
       " tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.5756, 0.4244, 0.0000, 0.0000],\n",
       "          [0.5429, 0.4571, 0.0000, 0.0000],\n",
       "          [0.5617, 0.4383, 0.0000, 0.0000],\n",
       "          [0.5537, 0.4463, 0.0000, 0.0000],\n",
       "          [0.5608, 0.4392, 0.0000, 0.0000]]], grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_attn(tgt, memory, memory, attn_mask=memory_mask, key_padding_mask=padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b6696f",
   "metadata": {},
   "source": [
    "the memory_mask is always be set to None in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3670f756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3237, -1.3233,  0.5419, -0.9428,  1.4005],\n",
      "         [ 0.2189,  0.1617,  0.3480, -1.8708,  1.1423],\n",
      "         [ 0.6429, -0.9477, -1.4305,  1.1287,  0.6066],\n",
      "         [-0.5365,  0.1947, -1.6440,  0.9944,  0.9914],\n",
      "         [ 1.9187, -0.0513, -0.5537, -0.3866, -0.9272],\n",
      "         [-1.5761,  0.8436,  0.7901,  0.7387, -0.7964]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([1, 6, 5])\n"
     ]
    }
   ],
   "source": [
    "out = decoder_layer.forward(tgt, memory,tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_padding_mask, memory_mask=memory_mask, memory_key_padding_mask=padding_mask)\n",
    "print(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf60153",
   "metadata": {},
   "source": [
    "## 1.5 torch.nn.TransformerDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fba05b",
   "metadata": {},
   "source": [
    "~~~\n",
    "class TransformerDecoder(Module):\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the inputs (and mask) through the decoder layer in turn.\n",
    "\n",
    "        Args:\n",
    "            tgt: the sequence to the decoder (required).\n",
    "            memory: the sequence from the last layer of the encoder (required).\n",
    "            tgt_mask: the mask for the tgt sequence (optional).\n",
    "            memory_mask: the mask for the memory sequence (optional).\n",
    "            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n",
    "            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        output = tgt\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, memory, tgt_mask=tgt_mask,\n",
    "                         memory_mask=memory_mask,\n",
    "                         tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                         memory_key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4a2717",
   "metadata": {},
   "source": [
    "## 1.6 nn.Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a700b941",
   "metadata": {},
   "source": [
    "~~~\n",
    "class Transformer(Module):\n",
    "    def __init__(self, d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,\n",
    "                 num_decoder_layers: int = 6, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation: str = \"relu\", custom_encoder: Optional[Any] = None, custom_decoder: Optional[Any] = None,\n",
    "                 layer_norm_eps: float = 1e-5) -> None:\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        if custom_encoder is not None:\n",
    "            self.encoder = custom_encoder\n",
    "        else:\n",
    "            encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps)\n",
    "            encoder_norm = LayerNorm(d_model, eps=layer_norm_eps)\n",
    "            self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        if custom_decoder is not None:\n",
    "            self.decoder = custom_decoder\n",
    "        else:\n",
    "            decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps)\n",
    "            decoder_norm = LayerNorm(d_model, eps=layer_norm_eps)\n",
    "            self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        \n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Optional[Tensor] = None, tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Take in and process masked source/target sequences.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder (required).\n",
    "            tgt: the sequence to the decoder (required).\n",
    "            src_mask: the additive mask for the src sequence (optional).\n",
    "            tgt_mask: the additive mask for the tgt sequence (optional).\n",
    "            memory_mask: the additive mask for the encoder output (optional).\n",
    "            src_key_padding_mask: the ByteTensor mask for src keys per batch (optional).\n",
    "            tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional).\n",
    "            memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            - src: :math:`(S, N, E)`.\n",
    "            - tgt: :math:`(T, N, E)`.\n",
    "            - src_mask: :math:`(S, S)`.\n",
    "            - tgt_mask: :math:`(T, T)`.\n",
    "            - memory_mask: :math:`(T, S)`.\n",
    "            - src_key_padding_mask: :math:`(N, S)`.\n",
    "            - tgt_key_padding_mask: :math:`(N, T)`.\n",
    "            - memory_key_padding_mask: :math:`(N, S)`.\n",
    "\n",
    "            Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked\n",
    "            positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
    "            while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
    "            are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "            is provided, it will be added to the attention weight.\n",
    "            [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by\n",
    "            the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero\n",
    "            positions will be unchanged. If a BoolTensor is provided, the positions with the\n",
    "            value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "\n",
    "            - output: :math:`(T, N, E)`.\n",
    "\n",
    "            Note: Due to the multi-head attention architecture in the transformer model,\n",
    "            the output sequence length of a transformer is same as the input sequence\n",
    "            (i.e. target) length of the decode.\n",
    "\n",
    "            where S is the source sequence length, T is the target sequence length, N is the\n",
    "            batch size, E is the feature number\n",
    "\n",
    "        Examples:\n",
    "            >>> output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        \"\"\"\n",
    "\n",
    "        if src.size(1) != tgt.size(1):\n",
    "            raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
    "\n",
    "        if src.size(2) != self.d_model or tgt.size(2) != self.d_model:\n",
    "            raise RuntimeError(\"the feature number of src and tgt must be equal to d_model\")\n",
    "\n",
    "        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask=memory_key_padding_mask)\n",
    "        return output\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37208b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = nn.Transformer(d_model=5, nhead=1, num_encoder_layers=1,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a9f74cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2965,  0.7936,  0.8327,  0.4980, -1.8278],\n",
      "         [-1.2585,  0.3553,  1.5216,  0.3167, -0.9351],\n",
      "         [-0.3392,  0.7469,  1.0419,  0.3218, -1.7714],\n",
      "         [-1.0772,  0.3735,  1.2926,  0.6688, -1.2577],\n",
      "         [-0.7090,  0.6794,  1.1446,  0.4681, -1.5831],\n",
      "         [-1.3177,  0.5440,  1.1414,  0.7098, -1.0775]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([1, 6, 5])\n"
     ]
    }
   ],
   "source": [
    "output = transformer_model.forward(src =src, tgt=tgt)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4673692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_mask = generate_subsequent_mask(4,4)\n",
    "tgt_mask = generate_subsequent_mask(6,6)\n",
    "memory_mask = generate_subsequent_mask(6,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0a0d2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False,  True,  True]])\n",
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "src_key_padding_mask = torch.tensor([[False, False, True,True]])\n",
    "print(src_key_padding_mask)\n",
    "print(src_key_padding_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71c650ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6])\n",
      "tensor([[False, False, False,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "tgt_key_padding_mask = torch.tensor([\n",
    "    [False, False, False, True,True,True]])\n",
    "print(tgt_key_padding_mask.shape)\n",
    "print(tgt_key_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "adab4617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False,  True,  True]])\n",
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "memory_key_padding_mask = src_key_padding_mask\n",
    "print(memory_key_padding_mask)\n",
    "print(memory_key_padding_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "460ab5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.2855,  0.4926,  1.5218,  0.1471, -0.8760],\n",
      "         [-1.0737,  0.8774,  1.4166, -0.2065, -1.0139],\n",
      "         [-1.1779,  0.2661,  1.6428,  0.1709, -0.9019],\n",
      "         [-1.1278,  0.4578,  1.5637,  0.1336, -1.0273],\n",
      "         [-0.6210,  0.1721,  1.7180,  0.0088, -1.2779],\n",
      "         [-0.2900, -0.3071,  1.9049, -0.2431, -1.0648]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([1, 6, 5])\n"
     ]
    }
   ],
   "source": [
    "output = transformer_model.forward(src =src, \n",
    "                                   tgt=tgt,\n",
    "                                   src_mask=src_mask,\n",
    "                                   tgt_mask=tgt_mask, \n",
    "                                   memory_mask=memory_mask,\n",
    "                                   src_key_padding_mask=src_key_padding_mask,\n",
    "                                  tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                  memory_key_padding_mask=memory_key_padding_mask)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d24b072",
   "metadata": {},
   "source": [
    "## appendix: a case study to show how to use mask and padding in translation in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0010b4",
   "metadata": {},
   "source": [
    "~~~\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7048cfc4",
   "metadata": {},
   "source": [
    "how to generate the mask and padding_mask matrix in this context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b8ca1",
   "metadata": {},
   "source": [
    "~~~\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407a21b9",
   "metadata": {},
   "source": [
    "1. pay attention to the memory_mask and memory_padding_mask in the train round\n",
    "2. pay attention to target_input and tgt_output in this context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e4e1c",
   "metadata": {},
   "source": [
    "~~~\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(train_dataloader))\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8af5c7",
   "metadata": {},
   "source": [
    "### answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1901d1",
   "metadata": {},
   "source": [
    "int the context of translation task\n",
    "1. the memory_mask = None\n",
    "2. memory_key_padding_mask = src_padding_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
